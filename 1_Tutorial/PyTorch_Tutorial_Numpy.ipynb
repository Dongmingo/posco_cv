{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Tutorial\n",
    "- Indexing 과 broadcasting 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array indexing\n",
    "* Numpy array의 indexing은 일반적인 list와 유사하다. \n",
    "* 단, indexing해서 분리한 array도 원래 array의 memory를 참조하기 때문에 변경할 때 유의하여야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.20.1'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(\n",
    "    [[1, 2, 3, 4],\n",
    "     [5, 6, 7, 8],\n",
    "     [9, 10, 11, 12]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [5 6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100   2   3   4]\n",
      " [  5   6   7   8]]\n"
     ]
    }
   ],
   "source": [
    "b[0, 0] = 100\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100   2   3   4]\n",
      " [  5   6   7   8]\n",
      " [  9  10  11  12]]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100   2   3   4]\n",
      " [  5   6   7   8]]\n"
     ]
    }
   ],
   "source": [
    "c = deepcopy(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200   2   3   4]\n",
      " [  5   6   7   8]]\n"
     ]
    }
   ],
   "source": [
    "c[0, 0] = 200\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100   2   3   4]\n",
      " [  5   6   7   8]]\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]] \n",
      "\n",
      "[[2 3]\n",
      " [6 7]]\n",
      "2\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print(a, '\\n')\n",
    "\n",
    "# [[2 3]\n",
    "#  [6 7]]\n",
    "# call-by-reference가 된 경우\n",
    "b = a[:2, 1:3]\n",
    "print(b)\n",
    "\n",
    "print(a[0, 1])\n",
    "b[0, 0] = 77    # b[0, 0] is the same piece of data as a[0, 1]\n",
    "print(a[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "## 실습\n",
    "# (H, W, C)\n",
    "a = np.random.randn(4, 5, 3)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "b = a[:, :, 0:1]\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Slicing을 할 때는 dimension이 낮아질 수 있다.\n",
    "- Slicing을 하는 방법에는 여러가지가 있는데, integer를 활용해 indexing을 할 때는 dimension이 낮아지고, slicing을 이용해 indexing 할 때는 dimension이 유지된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]] (3, 4)\n",
      "___________________________\n",
      "Slicing Row\n",
      "___________________________\n",
      "[5 6 7 8] (4,) \n",
      "\n",
      "[[5 6 7 8]] (1, 4) \n",
      "\n",
      "[[5 6 7 8]] (1, 4) \n",
      "\n",
      "___________________________\n",
      "Slicing Column\n",
      "___________________________\n",
      "[ 2  6 10] (3,) \n",
      "\n",
      "[[ 2]\n",
      " [ 6]\n",
      " [10]] (3, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create the following rank 2 array with shape (3, 4)\n",
    "a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print(a, a.shape)\n",
    "\n",
    "row_r1 = a[1, :]    # Rank 1 view of the second row of a  \n",
    "row_r2 = a[1:2, :]  # Rank 2 view of the second row of a\n",
    "row_r3 = a[[1], :]  # Rank 2 view of the second row of a\n",
    "\n",
    "print(\"___________________________\")\n",
    "print(\"Slicing Row\")\n",
    "print(\"___________________________\")\n",
    "\n",
    "print(row_r1, row_r1.shape, '\\n')\n",
    "print(row_r2, row_r2.shape, '\\n')\n",
    "print(row_r3, row_r3.shape, '\\n')\n",
    "\n",
    "\n",
    "col_r1 = a[:, 1]\n",
    "col_r2 = a[:, 1:2]\n",
    "\n",
    "print(\"___________________________\")\n",
    "print(\"Slicing Column\")\n",
    "print(\"___________________________\")\n",
    "print(col_r1, col_r1.shape, '\\n')\n",
    "print(col_r2, col_r2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 슬라이싱을 만들 때 주의해야 할 점은 슬라이싱 된 배열은 원본 배열과 같은 데이터를 참조하기 때문에 슬라이싱 된 배열을 수정하면 원본 배열 역시 수정됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[[6]\n",
      " [9]]\n",
      "[[10]\n",
      " [ 9]]\n",
      "[[ 1  2  3]\n",
      " [ 4  5 10]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "print(a)\n",
    "c = a[1:3, 2:3]\n",
    "print(c)\n",
    "c[0] = 10\n",
    "print(c)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integer array를 이용해 indexing을 할 수 있다. \n",
    "- Slicing을 할 때는 네모난 subarray만 추출할 수 있지만, integer array를 이용할 경우 임의의 수치들을 꺼내올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "(3, 2)\n",
      "Solution 1 :  [1 4 5]\n",
      "Solution 2 :  [1 4 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([\n",
    "    [1,2], \n",
    "    [3,4], \n",
    "    [5,6]\n",
    "])\n",
    "print(a)\n",
    "print(a.shape)\n",
    "## [1, 4, 5]를 만들 수 있는 방법 두 가지 \n",
    "\n",
    "## Solution 1\n",
    "print(\"Solution 1 : \", np.array([a[0, 0], a[1, 1], a[2, 0]]))\n",
    "\n",
    "## Solution 2\n",
    "print(\"Solution 2 : \", a[[0, 1, 2], [0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[ 1  6  7 11]\n"
     ]
    }
   ],
   "source": [
    "# Create a new array from which we will select elements\n",
    "a = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "print(a)\n",
    "\n",
    "## TO DO \n",
    "\n",
    "# Select one element from each row of a using the indices\n",
    "b = np.array([0, 2, 0, 1])\n",
    "\n",
    "# Prints \"[ 1 6 7 11]\"\n",
    "print(a[np.arange(4), b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Boolean array로도 indexing을 할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False]\n",
      " [False False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "mask = a > 5\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[ 6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "b = a[mask]\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "a[mask] = 0\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.64662588  0.07274908]\n",
      " [ 0.32312858  0.58273996]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(2, 2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.07274908]\n",
      " [0.32312858 0.58273996]]\n"
     ]
    }
   ],
   "source": [
    "a[a < 0] = 0\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]]\n",
      "[ 3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "print(a > 2)\n",
    "print(a[a > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "- Broadcasting is strong!\n",
    "- Broadcasting은 우리가 잘아는 방송과 관련된 뜻이고 유사한 의미로, broadcast란 단어는 무언가를 '흩뿌리고 퍼뜨리고 전파'할 때 사용하는 단어이다. 아래 실습을 해보면서 전파한다는 의미를 더 설명해보도록 하겠다.\n",
    "\n",
    "## Two tensors are “broadcastable” if the following rules hold:\n",
    "\n",
    "- 1. Each tensor has at least one dimension.\n",
    "- 2. When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10 20 30]]\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "b = np.array([[10, 20, 30]])\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11 22 33]\n",
      " [14 25 36]]\n"
     ]
    }
   ],
   "source": [
    "c = a + b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  2  4]\n",
      " [ 5  5  7]\n",
      " [ 8  8 10]\n",
      " [11 11 13]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "v = np.array([1, 0, 1])\n",
    "y = np.empty_like(x)   \n",
    "\n",
    "## first solution\n",
    "for i in range(4):\n",
    "    y[i, :] = x[i, :] + v\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1]\n",
      " [1 0 1]\n",
      " [1 0 1]\n",
      " [1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([1, 0, 1])\n",
    "vv = np.tile(v, (4, 1))\n",
    "print(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  2  4]\n",
      " [ 5  5  7]\n",
      " [ 8  8 10]\n",
      " [11 11 13]]\n"
     ]
    }
   ],
   "source": [
    "## Second Solution\n",
    "x = np.array([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "v = np.array([1, 0, 1])\n",
    "vv = np.tile(v, (4, 1))  # Stack 4 copies of v on top of each other\n",
    "y = x + vv\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(1, 3)\n",
      "(4, 3)\n",
      "[[ 2  2  4]\n",
      " [ 5  5  7]\n",
      " [ 8  8 10]\n",
      " [11 11 13]]\n"
     ]
    }
   ],
   "source": [
    "## Third Solution\n",
    "v = np.array([1, 0, 1])\n",
    "print(v.shape)\n",
    "v = np.expand_dims(v, axis=0)\n",
    "print(v.shape)\n",
    "vv = np.repeat(v, repeats=4, axis=0)  # Stack 4 copies of v on top of each other\n",
    "print(vv.shape)\n",
    "y = x + vv  \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "(1, 3)\n",
      "[[ 2  2  4]\n",
      " [ 5  5  7]\n",
      " [ 8  8 10]\n",
      " [11 11 13]]\n"
     ]
    }
   ],
   "source": [
    "## Fourth Solution - Broad Casting\n",
    "print(x.shape)\n",
    "print(v.shape)\n",
    "y = x + v  # Add v to each row of x using broadcasting\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  2  4]\n",
      " [ 5  5  7]\n",
      " [ 8  8 10]\n",
      " [11 11 13]]\n",
      "[[ 2  2  4]\n",
      " [ 5  5  7]\n",
      " [ 8  8 10]\n",
      " [11 11 13]]\n",
      "[[ 2  2  4]\n",
      " [ 5  5  7]\n",
      " [ 8  8 10]\n",
      " [11 11 13]]\n",
      "[[ 2  2  4]\n",
      " [ 5  5  7]\n",
      " [ 8  8 10]\n",
      " [11 11 13]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
    "v = np.array([1, 0, 1])\n",
    "y = np.empty_like(x)   \n",
    "\n",
    "## first solution\n",
    "for i in range(4):\n",
    "    y[i, :] = x[i, :] + v\n",
    "print(y)\n",
    "\n",
    "## Second Solution\n",
    "v = np.array([1, 0, 1])\n",
    "vv = np.tile(v, (4, 1))  # Stack 4 copies of v on top of each other\n",
    "y = x + vv  \n",
    "print(y)\n",
    "\n",
    "## Third Solution\n",
    "v = np.array([1, 0, 1])\n",
    "v = np.expand_dims(v, axis=0)\n",
    "vv = np.repeat(v, repeats=4, axis=0)  # Stack 4 copies of v on top of each other\n",
    "y = x + vv  \n",
    "print(y)\n",
    "\n",
    "## Fourth Solution - Broad Casting\n",
    "y = x + v  # Add v to each row of x using broadcasting\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem 1 :  False\n",
      "problem 2 :  True\n",
      "problem 3 :  True\n",
      "problem 4 :  True\n",
      "problem 5 :  False\n"
     ]
    }
   ],
   "source": [
    "## Broadcasting possible cases\n",
    "def checkbroadcasting(x, y):\n",
    "    try:\n",
    "        x+y\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "x=np.empty((0))\n",
    "y=np.empty((2,2))\n",
    "print(\"problem 1 : \", checkbroadcasting(x,y))\n",
    "\n",
    "x=np.empty((2))\n",
    "y=np.empty((2,2))\n",
    "print(\"problem 2 : \", checkbroadcasting(x,y))\n",
    " \n",
    "x=np.empty((5,3,4,1))\n",
    "y=np.empty((3,4,1))\n",
    "print(\"problem 3 : \", checkbroadcasting(x,y))\n",
    "\n",
    "x=np.empty((5,3,4,1))\n",
    "y=np.empty((3,1,1))\n",
    "print(\"problem 4 : \", checkbroadcasting(x,y))\n",
    "\n",
    "x=np.empty((5,2,4,1))\n",
    "y=np.empty((3,1,1))\n",
    "print(\"problem 5 : \", checkbroadcasting(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "(2, 2)\n",
      "problem 1 :  False\n"
     ]
    }
   ],
   "source": [
    "x=np.empty((0))\n",
    "print(x.shape)\n",
    "y=np.empty((2,2))\n",
    "print(y.shape)\n",
    "print(\"problem 1 : \", checkbroadcasting(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 2)\n",
      "problem 2 :  True\n",
      "[[2 4]\n",
      " [4 6]]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([1, 2])\n",
    "print(x.shape)\n",
    "y=np.array([[1, 2], [3, 4]])\n",
    "print(y.shape)\n",
    "print(\"problem 2 : \", checkbroadcasting(x,y))\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem 3 :  True\n"
     ]
    }
   ],
   "source": [
    "x=np.empty((5,3,4,1))\n",
    "y=np.empty((3,4,1))\n",
    "print(\"problem 3 : \", checkbroadcasting(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem 4 :  True\n"
     ]
    }
   ],
   "source": [
    "x=np.empty((5,3,4,1))\n",
    "y=np.empty((3,1,1))\n",
    "print(\"problem 4 : \", checkbroadcasting(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem 5 :  True\n"
     ]
    }
   ],
   "source": [
    "x=np.empty((5,3,3,1))\n",
    "y=np.empty((3,4))\n",
    "print(\"problem 5 : \", checkbroadcasting(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem 6 :  True\n"
     ]
    }
   ],
   "source": [
    "x=np.empty((5,2,4,1))\n",
    "y=np.empty((1))\n",
    "print(\"problem 6 : \", checkbroadcasting(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지금까지 배운 Numpy에서의 indexing 과 Broadcasting 방법이 모두 Pytorch에도 적용 된다.\n",
    "\n",
    "## 그럼 왜 PyTorch를 사용하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "* Tensorflow의 Tensor와 다르지 않다.\n",
    "  * Numpy의 ndarrays를 기본적으로 활용하고 있다.\n",
    "  * Numpy의 ndarrays의 대부분의 operation을 사용할 수 있도록 구성되어 있다.\n",
    "* Numpy의 operation은 CPU만을 이용해 느리지만 Tensor는 CUDA를 활용해 GPU를 이용하기 때문에 빠르게 연산을 진행할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.5739e+04,  4.5849e-41, -7.5739e+04],\n",
      "        [ 4.5849e-41,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  1.4013e-45],\n",
      "        [ 0.0000e+00,  1.4013e-45,  0.0000e+00],\n",
      "        [ 1.4013e-45,  0.0000e+00,  1.4013e-45]]) \n",
      "\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5, 3)\n",
    "print(x, '\\n')\n",
    "print(x.shape)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 4, 5],\n",
      "        [1, 2, 3]]) \n",
      "\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Construct a matrix with the list\n",
    "x = torch.tensor([[3, 4, 5], [1, 2, 3]])\n",
    "print(x, '\\n')\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9723, 0.6899, 0.0356],\n",
      "        [0.6539, 0.2133, 0.7549],\n",
      "        [0.5915, 0.6877, 0.6221],\n",
      "        [0.6352, 0.4728, 0.5338],\n",
      "        [0.4593, 0.0836, 0.8070]]) \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Construct a randomly initialized matrix \n",
    "x = torch.rand(5, 3) # np.random.rand\n",
    "print(x, '\\n')\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.5739e+04,  4.5849e-41, -5.7093e-13],\n",
      "        [ 3.0785e-41,  6.6741e+22,  4.3953e-11],\n",
      "        [ 1.7254e+19,  5.0284e-14,  1.1704e-19],\n",
      "        [ 1.3563e-19,  2.9503e-39,  4.5849e-41],\n",
      "        [-1.0803e+05,  4.5849e-41,  2.7045e-43]]) \n",
      "\n",
      "tensor([[0.0020, 0.7242, 0.1427],\n",
      "        [0.7630, 0.9375, 0.8915],\n",
      "        [0.5187, 0.9806, 0.6207],\n",
      "        [0.2121, 0.3891, 0.6553],\n",
      "        [0.9884, 0.2705, 0.8285]]) \n",
      "\n",
      "tensor([[3, 4, 5],\n",
      "        [1, 2, 3]]) \n",
      "\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Construct a 5 x 3 matrix, uninitialized (random initialized)\n",
    "x = torch.Tensor(5, 3)\n",
    "print(x, '\\n')\n",
    "\n",
    "# Construct a randomly initialized matrix \n",
    "x = torch.rand(5, 3)\n",
    "print(x, '\\n')\n",
    "\n",
    "# Construct a matrix with the list\n",
    "x = torch.tensor([[3, 4, 5], [1, 2, 3]])\n",
    "print(x, '\\n')\n",
    "\n",
    "# Get its size\n",
    "print(x.size())\n",
    "print(x.shape)\n",
    "\n",
    "# Get its grad\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dtype and device \n",
    " * dtype - Tensor의 데이터 타입\n",
    " * device - Tensor의 작업 위치 (cpu or cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4., 5.],\n",
      "        [1., 2., 3.]], dtype=torch.float64) \n",
      "\n",
      "tensor([[3, 4, 5],\n",
      "        [1, 2, 3]], dtype=torch.int32) \n",
      "\n",
      "tensor([[ 6.,  8., 10.],\n",
      "        [ 2.,  4.,  6.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[3, 4, 5], [1, 2, 3]], dtype=torch.float64)\n",
    "print(x, '\\n')\n",
    "\n",
    "y = torch.tensor([[3, 4, 5], [1, 2, 3]], dtype=torch.int)\n",
    "print(y, '\\n')\n",
    "\n",
    "\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4., 5.],\n",
      "        [1., 2., 3.]]) \n",
      "\n",
      "torch.float32\n",
      "tensor([[3., 4., 5.],\n",
      "        [1., 2., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[3, 4, 5], [1, 2, 3]], dtype=torch.float32)\n",
    "print(x, '\\n')\n",
    "print(x.dtype)\n",
    "y = x.double()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4., 5.],\n",
      "        [1., 2., 3.]], dtype=torch.float64) \n",
      "\n",
      "tensor([[ 6.,  8., 10.],\n",
      "        [ 2.,  4.,  6.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = y.double() \n",
    "print(y, '\\n')\n",
    "\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[3, 4, 5], [1, 2, 3]], dtype=torch.float32)\n",
    "print(x.device)\n",
    "x = x.to(torch.device('cuda'))\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4., 5.],\n",
      "        [1., 2., 3.]], device='cuda:0') \n",
      "\n",
      "cuda:0 \n",
      "\n",
      "tensor([[3., 4., 5.],\n",
      "        [1., 2., 3.]], device='cuda:1') \n",
      "\n",
      "cuda:1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') # 'cpu'\n",
    "x = x.to(device)\n",
    "print(x, '\\n')\n",
    "print(x.device, '\\n')\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "x = x.to(device)\n",
    "\n",
    "print(x, '\\n')\n",
    "print(x.device, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(x.device)\n",
    "x = x.to(torch.device('cpu'))\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "x = x.cuda()\n",
    "print(x.device)\n",
    "x = x.cpu()\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before \"to\" method\n",
      "torch.float64 cpu\n",
      "torch.float32 cpu\n",
      "torch.int32 cuda:1 \n",
      "\n",
      "After \"to\" method\n",
      "torch.int32 cuda:0\n",
      "torch.int32 cuda:1\n",
      "torch.int32 cpu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "device_0 = torch.device('cuda:0')\n",
    "device_1 = torch.device('cuda:1')\n",
    "\n",
    "x = torch.randn(4, 3, dtype=torch.float64)\n",
    "y = torch.randn(4, 3, dtype=torch.float32)\n",
    "z = torch.randint(0, 10, (4, 3), dtype=torch.int32)\n",
    "\n",
    "z = z.to(device_1)\n",
    "\n",
    "print('Before \"to\" method')\n",
    "\n",
    "print(x.dtype, x.device)\n",
    "print(y.dtype, y.device)\n",
    "print(z.dtype, z.device, '\\n')\n",
    "\n",
    "\n",
    "print('After \"to\" method')\n",
    "# to method with specific dtype and device \n",
    "x = x.to(dtype=torch.int32, device=device_0)\n",
    "\n",
    "# to method with some tensor \n",
    "y = y.to(z)\n",
    "z = z.to(device='cpu')\n",
    "\n",
    "print(x.dtype, x.device)\n",
    "print(y.dtype, y.device)\n",
    "print(z.dtype, z.device, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing like Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 1.4013e-45, 1.2612e-44, 1.4013e-45],\n",
      "        [9.8091e-45, 1.1210e-44, 1.2612e-44, 4.2039e-45, 5.6052e-45],\n",
      "        [8.4078e-45, 1.1210e-44, 8.9683e-44, 0.0000e+00, 1.5835e-43]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 5)\n",
    "print(x, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]) \n",
      "\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]) \n",
      "\n",
      "tensor([[3.1415, 3.1415, 3.1415, 3.1415, 3.1415],\n",
      "        [3.1415, 3.1415, 3.1415, 3.1415, 3.1415],\n",
      "        [3.1415, 3.1415, 3.1415, 3.1415, 3.1415]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(3, 5)\n",
    "print(x, '\\n')\n",
    "\n",
    "x = torch.ones(3, 5)\n",
    "print(x, '\\n')\n",
    "\n",
    "x = torch.full((3, 5), 3.1415)\n",
    "print(x, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 5, 1)\n",
    "print(x, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.6250, 1.2500, 1.8750, 2.5000, 3.1250, 3.7500, 4.3750, 5.0000]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = torch.linspace(0, 5, 9)\n",
    "print(y, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = torch.logspace(-10, 10, 5)\n",
    "print(z, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = torch.eye(5) # I: Identity Matrix\n",
    "print(z, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.5739e+04,  4.5849e-41,  9.0966e-23,  3.0787e-41,  1.4013e-45],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  9.0937e-23,  3.0787e-41, -2.0974e-13]]) \n",
      "\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]) \n",
      "\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]) \n",
      "\n",
      "tensor([[3.1415, 3.1415, 3.1415, 3.1415, 3.1415],\n",
      "        [3.1415, 3.1415, 3.1415, 3.1415, 3.1415],\n",
      "        [3.1415, 3.1415, 3.1415, 3.1415, 3.1415]]) \n",
      "\n",
      "tensor([0, 2, 4]) \n",
      "\n",
      "tensor([0.0000, 0.6250, 1.2500, 1.8750, 2.5000, 3.1250, 3.7500, 4.3750, 5.0000]) \n",
      "\n",
      "tensor([1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10]) \n",
      "\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]]) \n",
      "\n",
      "tensor([[8, 3, 9, 3, 7],\n",
      "        [6, 8, 5, 8, 7],\n",
      "        [8, 5, 9, 4, 5]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 5)\n",
    "print(x, '\\n')\n",
    "\n",
    "x = torch.zeros(3, 5)\n",
    "print(x, '\\n')\n",
    "\n",
    "x = torch.ones(3, 5)\n",
    "print(x, '\\n')\n",
    "\n",
    "x = torch.full((3, 5), 3.1415)\n",
    "print(x, '\\n')\n",
    "\n",
    "x = torch.arange(0, 5, 2)\n",
    "print(x, '\\n')\n",
    "\n",
    "y = torch.linspace(0, 5, 9)\n",
    "print(y, '\\n')\n",
    "\n",
    "z = torch.logspace(-10, 10, 5)\n",
    "print(z, '\\n')\n",
    "\n",
    "z = torch.eye(5)\n",
    "print(z, '\\n')\n",
    "\n",
    "# Construct a 3 x 5 matrix with random value from uniform distribution, i.e. Uniform[0, 1)\n",
    "x = torch.rand(3, 5)\n",
    "\n",
    "# Construct a 3 x 5 matrix with random value from normal distribution, i.e. Normal(0, 1)\n",
    "x = torch.randn(3, 5)\n",
    "\n",
    "\n",
    "x = torch.randint(3, 10, (3, 5))\n",
    "print(x, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*\\_like function and new\\_\\* function\n",
    " * \\*\\_like: Tensor를 input으로 받아, Tensor 모양의 matrix를 return.\n",
    " * new\\_\\*: Shape를 input으로 받아, Tensor와 같은 type과 device를 가지는 matrix를 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "cuda:0\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 5)\n",
    "x = x.cuda()\n",
    "y = x.new_zeros(2, 3)\n",
    "# y = torch.zeros_like(x)\n",
    "print(y.shape)\n",
    "print(y.device)\n",
    "print(y)\n",
    "print(x.device)\n",
    "print(y.device)\n",
    "# z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]) \n",
      "\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]]) \n",
      "\n",
      "torch.int64 cpu\n",
      "torch.int64 cpu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = torch.zeros_like(x)\n",
    "print(y, '\\n')\n",
    "\n",
    "# Make zero matrix with attribute of x\n",
    "z = x.new_zeros(2, 3)\n",
    "print(z, '\\n')\n",
    "print(x.dtype, x.device)\n",
    "print(z.dtype, z.device,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From numpy to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      " [1. 1. 1. 1. 1.] \n",
      " tensor([1., 1., 1., 1., 1.], dtype=torch.float64) \n",
      " [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(b.device)\n",
    "c = b.numpy()\n",
    "print(type(c))\n",
    "print(\"\\n\",a,\"\\n\",b,\"\\n\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations\n",
    "* Operations에도 여러가지 syntax가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3943, 1.7993, 1.6014],\n",
      "        [1.4621, 1.6473, 0.8761],\n",
      "        [1.2929, 1.2117, 1.3055],\n",
      "        [1.2954, 1.2846, 0.8653],\n",
      "        [0.7702, 1.7095, 1.1845]])\n"
     ]
    }
   ],
   "source": [
    "hyundai = x + y\n",
    "print(hyundai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2698, 0.1173, 0.1252],\n",
      "        [0.9326, 0.1908, 0.5700],\n",
      "        [0.7868, 0.4511, 0.0069],\n",
      "        [0.1050, 0.0431, 0.0550],\n",
      "        [0.5286, 0.7321, 0.0831]])\n",
      "tensor([[0.2708, 0.8525, 0.1572],\n",
      "        [1.7038, 0.9540, 1.4408],\n",
      "        [1.3457, 0.6580, 0.7785],\n",
      "        [0.1549, 0.8289, 0.9555],\n",
      "        [1.0819, 0.7779, 0.3125]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(y)\n",
    "# z = x + y\n",
    "y.add_(x)\n",
    "print(y)\n",
    "# y.add_(x)\n",
    "# print(\"solution 1 : \", x + y, '\\n')\n",
    "# print(\"solution 1 : \", torch.add(x, y), '\\n')\n",
    "\n",
    "# result = torch.Tensor(5, 3)\n",
    "# torch.add(x, y, out=result)\n",
    "# print(\"solution 3 : \", result, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution 1 :  tensor([[1.3333, 0.5676, 0.5711],\n",
      "        [0.6093, 0.6187, 1.2319],\n",
      "        [1.3260, 0.3188, 0.8614],\n",
      "        [1.2587, 0.2385, 1.4925],\n",
      "        [0.0774, 1.2341, 1.2397]]) \n",
      "\n",
      "solution 2 :  tensor([[1.3333, 0.5676, 0.5711],\n",
      "        [0.6093, 0.6187, 1.2319],\n",
      "        [1.3260, 0.3188, 0.8614],\n",
      "        [1.2587, 0.2385, 1.4925],\n",
      "        [0.0774, 1.2341, 1.2397]]) \n",
      "\n",
      "solution 3 :  tensor([[1.3333, 0.5676, 0.5711],\n",
      "        [0.6093, 0.6187, 1.2319],\n",
      "        [1.3260, 0.3188, 0.8614],\n",
      "        [1.2587, 0.2385, 1.4925],\n",
      "        [0.0774, 1.2341, 1.2397]]) \n",
      "\n",
      "solution 4 :  tensor([[1.3333, 0.5676, 0.5711],\n",
      "        [0.6093, 0.6187, 1.2319],\n",
      "        [1.3260, 0.3188, 0.8614],\n",
      "        [1.2587, 0.2385, 1.4925],\n",
      "        [0.0774, 1.2341, 1.2397]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "print(\"solution 1 : \", x + y, '\\n')\n",
    "\n",
    "\n",
    "print(\"solution 2 : \", torch.add(x, y), '\\n')\n",
    "\n",
    "\n",
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(\"solution 3 : \", result, '\\n')\n",
    "\n",
    "y.add_(x)\n",
    "print(\"solution 4 : \", y, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Same indexing as numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0010, 0.7352, 0.0320],\n",
      "        [0.7713, 0.7631, 0.8707],\n",
      "        [0.5590, 0.2069, 0.7716],\n",
      "        [0.0499, 0.7859, 0.9005],\n",
      "        [0.5534, 0.0459, 0.2294]])\n",
      "tensor([0.7352, 0.7631, 0.2069, 0.7859, 0.0459]) \n",
      "\n",
      "tensor([[False,  True, False],\n",
      "        [ True,  True,  True],\n",
      "        [ True, False,  True],\n",
      "        [False,  True,  True],\n",
      "        [ True, False, False]])\n",
      "tensor([0.7352, 0.7713, 0.7631, 0.8707, 0.5590, 0.7716, 0.7859, 0.9005, 0.5534])\n"
     ]
    }
   ],
   "source": [
    "# indexing 또한 비슷하게\n",
    "print(x)\n",
    "print(x[:, 1], '\\n')\n",
    "print(x>0.5)\n",
    "print(x[x > 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape and view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3])\n",
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]])\n",
      "tensor([[1],\n",
      "        [4],\n",
      "        [7]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
    "print(x.shape)\n",
    "print(x)\n",
    "y = x.view(3, 1, 3)\n",
    "print(y[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
      "\n",
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]]) \n",
      "\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 10)\n",
    "print(x, '\\n')\n",
    "\n",
    "y = x.reshape(2, 5)\n",
    "z = y.reshape(10,)\n",
    "print(y, '\\n')\n",
    "print(z)\n",
    "\n",
    "# y[0, 0] = 3\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \n",
      "\n",
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]]) \n",
      "\n",
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29]]) \n",
      "\n",
      "torch.Size([5, 6]) \n",
      "\n",
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9]],\n",
      "\n",
      "        [[10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29]]]) \n",
      "\n",
      "torch.Size([3, 2, 5]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the shape of tensor \n",
    "x = torch.arange(0, 10)\n",
    "print(x, '\\n')\n",
    "\n",
    "y = x.view(2, 5)\n",
    "print(y, '\\n')\n",
    "\n",
    "x = torch.arange(0, 30).view(5, 6)\n",
    "print(x, '\\n')\n",
    "print(x.size(), '\\n')\n",
    "\n",
    "y = x.view(-1, 2, 5)\n",
    "print(y, '\\n')\n",
    "print(y.size(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1, 2, 3, 4],\n",
      "         [5, 6, 7, 8, 9]]])\n",
      "torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "x = torch.arange(0, 10)\n",
    "x = x.reshape(-1,2,5)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions from the last section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. tensorflow: numpy to tf tensor\n",
    "-  tf.convert_to_tensor() in tensorflow>=2.0\n",
    "\n",
    "### Q2. view vs. reshape\n",
    "- The only difference is that reshape might copy the input tensor.\n",
    "- This is related to the contiguity of the input tensor.\n",
    "- \"view\" function only works if and only if the input tensor is contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "True\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "print(x)\n",
    "print(x.is_contiguous())\n",
    "z = x.view(3, 2)\n",
    "print(z)\n",
    "z2 = x.reshape(3, 2)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "False\n",
      "tensor([[1, 4, 2],\n",
      "        [5, 3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "y = x.t()\n",
    "print(y)\n",
    "print(y.is_contiguous())\n",
    "z = y.reshape(2, 3)\n",
    "print(z)\n",
    "z[0] = 10\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transpose and permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1039, 0.0572, 0.3595],\n",
      "         [0.4991, 0.5807, 0.7730]]])\n",
      "tensor([[[0.1039],\n",
      "         [0.4991]],\n",
      "\n",
      "        [[0.0572],\n",
      "         [0.5807]],\n",
      "\n",
      "        [[0.3595],\n",
      "         [0.7730]]])\n",
      "True\n",
      "tensor([[[0.1039],\n",
      "         [0.4991]],\n",
      "\n",
      "        [[0.0572],\n",
      "         [0.5807]],\n",
      "\n",
      "        [[0.3595],\n",
      "         [0.7730]]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 2, 3)\n",
    "print(x)\n",
    "y = torch.transpose(x, 0, 2)  ## dim0 dim1 바꾸고 싶을 때\n",
    "print(y)\n",
    "y = y.contiguous()\n",
    "print(y.is_contiguous())\n",
    "z = x.permute(2, 1, 0)  ## dim0 dim1 ~ dimn 바꾸고 싶을 때\n",
    "print(z)\n",
    "print(z.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7159, 0.9104, 0.3810, 0.4249, 0.6850],\n",
      "        [0.6222, 0.9410, 0.8945, 0.7266, 0.7523],\n",
      "        [0.2084, 0.6476, 0.4023, 0.8560, 0.1249],\n",
      "        [0.5671, 0.9291, 0.4229, 0.6683, 0.2911]])\n",
      "tensor([[0.7159, 0.9104, 0.3810, 0.4249, 0.6850],\n",
      "        [0.6222, 0.9410, 0.8945, 0.7266, 0.7523],\n",
      "        [0.2084, 0.6476, 0.4023, 0.8560, 0.1249],\n",
      "        [0.5671, 0.9291, 0.4229, 0.6683, 0.2911]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 4, 5)\n",
    "print(x[0, :, :])\n",
    "y = torch.transpose(x, 0, 1)\n",
    "print(y[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### squeeze and unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 20, 128])\n",
      "torch.Size([20, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 1, 20, 128)\n",
    "print(x.shape)\n",
    "x = x.squeeze() # [1, 1, 20, 128] -> [20, 128]\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 20, 128])\n",
      "torch.Size([1, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "x2 = torch.rand(1, 1, 20, 128)\n",
    "print(x2.shape)\n",
    "x2 = x2.squeeze(dim=1) # [1, 1, 20, 128] -> [1, 20, 128]\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 128])\n",
      "torch.Size([1, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "x3 = x.unsqueeze(0)\n",
    "print(x3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 20, 128])\n",
      "torch.Size([20, 128])\n",
      "torch.Size([1, 1, 20, 128])\n",
      "torch.Size([1, 20, 128])\n",
      "torch.Size([1, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 1, 20, 128)\n",
    "print(x.shape)\n",
    "x = x.squeeze() # [1, 1, 20, 128] -> [20, 128]\n",
    "print(x.shape)\n",
    "x2 = torch.rand(1, 1, 20, 128)\n",
    "print(x2.shape)\n",
    "x2 = x2.squeeze(dim=1) # [1, 1, 20, 128] -> [1, 20, 128]\n",
    "print(x2.shape)\n",
    "\n",
    "x3 = x.unsqueeze(0)\n",
    "print(x3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiplication and concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.],\n",
      "        [6., 6., 6.],\n",
      "        [6., 6., 6.],\n",
      "        [6., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 3)+1\n",
    "y = torch.ones(5, 3)+2\n",
    "z = x * y\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n",
      "tensor([[18., 18., 18., 18., 18.],\n",
      "        [18., 18., 18., 18., 18.],\n",
      "        [18., 18., 18., 18., 18.],\n",
      "        [18., 18., 18., 18., 18.],\n",
      "        [18., 18., 18., 18., 18.]]) torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "## matrix multiplication\n",
    "## y = W.T * x + b\n",
    "z= torch.matmul(x, y.t())\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(z, z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "tensor([[2., 2., 2., 3., 3., 3.],\n",
      "        [2., 2., 2., 3., 3., 3.],\n",
      "        [2., 2., 2., 3., 3., 3.],\n",
      "        [2., 2., 2., 3., 3., 3.],\n",
      "        [2., 2., 2., 3., 3., 3.]])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n",
      "torch.Size([5, 6])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)\n",
    "z = torch.cat([x, y], dim=1)\n",
    "print(z)\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([5, 3])\n",
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.],\n",
      "        [6., 6., 6.],\n",
      "        [6., 6., 6.],\n",
      "        [6., 6., 6.]]) torch.Size([5, 3])\n",
      "tensor([[18., 18., 18., 18., 18.],\n",
      "        [18., 18., 18., 18., 18.],\n",
      "        [18., 18., 18., 18., 18.],\n",
      "        [18., 18., 18., 18., 18.],\n",
      "        [18., 18., 18., 18., 18.]]) torch.Size([5, 5])\n",
      "torch.Size([1, 5, 3]) torch.Size([1, 5, 3])\n",
      "torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "## element wise multiplication\n",
    "x = torch.ones(5, 3)+1\n",
    "y = torch.ones(5, 3)+2\n",
    "z = x * y\n",
    "print(x.shape, y.shape)\n",
    "print(z, z.shape)\n",
    "\n",
    "## matrix multiplication\n",
    "z= torch.matmul(x, y.t())\n",
    "print(z, z.shape)\n",
    "\n",
    "## concat\n",
    "x = x.unsqueeze(0)\n",
    "y = y.unsqueeze(0)\n",
    "print(x.shape, y.shape)\n",
    "z = torch.cat([x, y], dim=0)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 넘파이의 다양한 operation들이 토치에 같은 함수나 변형된 함수로 대부분 탑재 되어있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\***********Numpy Practice Time\\***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch의 Autograd: automatic differentiation\n",
    "* Autograd package는 Tensors가 사용할 수 있는 모든 Operation의 Gradient를 자동으로 계산해준다.\n",
    "* Tensor의 required_grad attribute를 이용해 gradient의 계산여부를 결정할 수 있다.\n",
    "  * 계산이 완료된 이후에 .backward()를 호출하면 자동으로 gradient를 계산한다.\n",
    "  * .grad attribute를 통해 마찬가지로 gradient에 접근할 수 있다. \n",
    "  * .grad_fn attribute를 통해 해당 Variable이 어떻게 생성되었는지 확인할 수 있다. 해당 값으로 해당 노드의 local gradient 구할 수 있게 됨.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create a variable\n",
    "# x = torch.ones(2, 2)\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "\n",
    "print(x)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.) None <MeanBackward0 object at 0x7fcef03f1880>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]]) None <MulBackward0 object at 0x7fcef03fbc40>\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]]) None <AddBackward0 object at 0x7fcef04573d0>\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]]) None None\n",
      "##############################\n",
      "tensor(27.) tensor(1.)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]]) tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]]) tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]]) tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "out.retain_grad()\n",
    "z.retain_grad()\n",
    "y.retain_grad()\n",
    "\n",
    "# y,z는 operation으로 생성된 결과이기 때문에 grad_fn이 있지만 , x는 없다.\n",
    "print(out.data, out.grad, out.grad_fn)\n",
    "print(z.data, z.grad, z.grad_fn)\n",
    "print(y.data, y.grad, y.grad_fn)\n",
    "print(x.data, x.grad, x.grad_fn)\n",
    "\n",
    "\n",
    "out.backward()\n",
    "\n",
    "print('##############################')\n",
    "print(out.data, out.grad)\n",
    "print(z.data, z.grad)\n",
    "print(y.data, y.grad)\n",
    "print(x.data, x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실제로 Gradient 를 계산하면 다음과 같다.\n",
    "$$o = \\frac{1}{4}\\sum_{i} z_{i}$$ \n",
    "\n",
    "$$z_{i}=3(x_{i}+2)^{2}$$\n",
    "\n",
    "$$z_{i}|_{x_{i}=1} = 27 $$\n",
    "\n",
    "$$ \\frac{\\partial o}{\\partial x_{i}} = \\frac{3}{2}(x_{i} + 2) $$\n",
    "\n",
    "$$ \\frac{\\partial o}{\\partial x_{i}}|_{x_{i}=1} = 4.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients \n",
    "* out.backward()을 하면 out의 gradient를 1로 시작해 Back-propagation을 시작한다.\n",
    "* .backward()를 호출한 이후부터는 .grad를 통해 각 변수의 gradient를 구할 수 있다.\n",
    "* https://teamdable.github.io/techblog/PyTorch-Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor(5.)\n",
      "y tensor(125.)\n",
      "z tensor(4.8283)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(5.0)\n",
    "y = x ** 3\n",
    "z = torch.log(y)\n",
    "\n",
    "print('x', x)\n",
    "print('y', y)\n",
    "print('z', z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x requires_grad(False) is_leaf(True) grad_fn(None) grad(None) tensor(tensor(5.))\n",
      "y requires_grad(False) is_leaf(True) grad_fn(None) grad(None) tensor(tensor(125.))\n",
      "z requires_grad(False) is_leaf(True) grad_fn(None) grad(None) tensor(tensor(4.8283))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)\n",
    "\n",
    "x = torch.tensor(5.0)\n",
    "y = x ** 3\n",
    "z = torch.log(y)\n",
    "\n",
    "print('x', get_tensor_info(x))\n",
    "print('y', get_tensor_info(y))\n",
    "print('z', get_tensor_info(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
      "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7fcef0457130>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7fcef03f1e20>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
      "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
      "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7fcf8f7c0fa0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7fcef0457d00>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "z = torch.log(y)\n",
    "\n",
    "print('x', get_tensor_info(x))\n",
    "print('y', get_tensor_info(y))\n",
    "print('z', get_tensor_info(z))\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print('x_after_backward', get_tensor_info(x))\n",
    "print('y_after_backward', get_tensor_info(y))\n",
    "print('z_after_backward', get_tensor_info(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_before_backward : requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
      "y_before_backward : requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7fcef03f1880>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z_before_backward : requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7fcfc03160a0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
      "x_after_backward : requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
      "y_after_backward : requires_grad(True) is_leaf(False) retains_grad(True) grad_fn(<PowBackward0 object at 0x7fcf8f81f130>) grad(0.00800000037997961) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z_after_backward : requires_grad(True) is_leaf(False) retains_grad(True) grad_fn(<LogBackward0 object at 0x7fcef03f13a0>) grad(1.0) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "z = torch.log(y)\n",
    "\n",
    "print('x_before_backward :', get_tensor_info(x))\n",
    "print('y_before_backward :', get_tensor_info(y))\n",
    "print('z_before_backward :', get_tensor_info(z))\n",
    "\n",
    "y.retain_grad()\n",
    "z.retain_grad()\n",
    "z.backward()\n",
    "\n",
    "print('x_after_backward :', get_tensor_info(x))\n",
    "print('y_after_backward :', get_tensor_info(y))\n",
    "print('z_after_backward :', get_tensor_info(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
      "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7fcef03f1970>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7fcfc0316100>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
      "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
      "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7fcef03f8370>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7fcf8f7de070>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-292-27b97fb5261f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z_after_backward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_tensor_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "z = torch.log(y)\n",
    "\n",
    "print('x', get_tensor_info(x))\n",
    "print('y', get_tensor_info(y))\n",
    "print('z', get_tensor_info(z))\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print('x_after_backward', get_tensor_info(x))\n",
    "print('y_after_backward', get_tensor_info(y))\n",
    "print('z_after_backward', get_tensor_info(z))\n",
    "\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(None) tensor(tensor(5., requires_grad=True))\n",
      "y requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7fcef04c1be0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7fcef03f8e50>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
      "x_after_backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(0.6000000238418579) tensor(tensor(5., requires_grad=True))\n",
      "y_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7fcef03f8310>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z_after_backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7fcef04c1970>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n",
      "x_after_2backward requires_grad(True) is_leaf(True) retains_grad(False) grad_fn(None) grad(1.2000000476837158) tensor(tensor(5., requires_grad=True))\n",
      "y_after_2backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<PowBackward0 object at 0x7fcef04c1be0>) grad(None) tensor(tensor(125., grad_fn=<PowBackward0>))\n",
      "z_after_2backward requires_grad(True) is_leaf(False) retains_grad(False) grad_fn(<LogBackward0 object at 0x7fcef03f81f0>) grad(None) tensor(tensor(4.8283, grad_fn=<LogBackward0>))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def get_tensor_info(tensor):\n",
    "  info = []\n",
    "  for name in ['requires_grad', 'is_leaf', 'retains_grad', 'grad_fn', 'grad']:\n",
    "    info.append(f'{name}({getattr(tensor, name, None)})')\n",
    "  info.append(f'tensor({str(tensor)})')\n",
    "  return ' '.join(info)\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x ** 3\n",
    "z = torch.log(y)\n",
    "\n",
    "print('x', get_tensor_info(x))\n",
    "print('y', get_tensor_info(y))\n",
    "print('z', get_tensor_info(z))\n",
    "\n",
    "z.backward(retain_graph=True)\n",
    "\n",
    "print('x_after_backward', get_tensor_info(x))\n",
    "print('y_after_backward', get_tensor_info(y))\n",
    "print('z_after_backward', get_tensor_info(z))\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print('x_after_2backward', get_tensor_info(x))\n",
    "print('y_after_2backward', get_tensor_info(y))\n",
    "print('z_after_2backward', get_tensor_info(z))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

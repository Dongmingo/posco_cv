{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b74ef3e-a0ad-473f-9abc-dc315b04391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# !pip install pytorch_metric_learning\n",
    "import PIL\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c706b768-fc36-49cd-9c56-20c7d30a3c82",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "##download dataset \n",
    "##밑에 코드 주석을 풀면 images.tgz 파일이 다운받아지고 images라는 폴더가 생깁니다. \n",
    "#https://drive.google.com/file/d/1o0grxoqCxcrgI6Nd4nxybXulU5IrE8wX/view?usp=sharing\n",
    "\n",
    "# !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1o0grxoqCxcrgI6Nd4nxybXulU5IrE8wX\" > /dev/null\n",
    "# !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1o0grxoqCxcrgI6Nd4nxybXulU5IrE8wX\" -o CUB.zip\n",
    "# !mkdir CUB\n",
    "# !unzip CUB.zip -d CUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b7b21a-590e-4052-b2d2-4f503412cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUBirds(Dataset):\n",
    "    def __init__(self, root, mode, transform = None):\n",
    "        ##implement this\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        \n",
    "        if self.mode == \"train\": #0~100\n",
    "            self.classes = range(0, 100)\n",
    "        elif self.mode == \"eval\":\n",
    "            self.classes = range(100, 200)\n",
    "        \n",
    "        self.ys = [] #label list\n",
    "        self.im_paths = [] #image path list\n",
    "    #folder\n",
    "        #class1\n",
    "            #1.jpg\n",
    "            #2.jpg\n",
    "        #class2\n",
    "            #1.jpg\n",
    "            #2.jpg     \n",
    "        image_folder = torchvision.datasets.ImageFolder(root = os.path.join(self.root, \"CUB/train\"))\n",
    "        \n",
    "        for i in image_folder.imgs:\n",
    "            # first index: image path, second index: label\n",
    "            y = i[1]\n",
    "            \n",
    "            if y in self.classes:\n",
    "                self.ys += [y]\n",
    "                self.im_paths.append(os.path.join(self.root, i[0]))\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        im_path = self.im_paths[index]\n",
    "        \n",
    "        im = PIL.Image.open(im_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "        \n",
    "        target = self.ys[index]\n",
    "        \n",
    "        return im, target\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c2d71b-a945-4d5c-9f39-cbb0e00a8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = CUBirds(\"./\", \"train\", None)\n",
    "# print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d4f128c-713a-4f7d-b05d-bc7c0a98d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(model, full_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Just a place holder for our 0th image embedding.\n",
    "    embeddings = torch.tensor([], dtype=torch.float)\n",
    "    \n",
    "    # Again we do not compute loss here so. No gradients.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (train_img, lbl) in enumerate(full_loader):\n",
    "\n",
    "            train_img = train_img.to(device)\n",
    "            \n",
    "            # Get encoder outputs and move outputs to cpu\n",
    "            enc_output = model(train_img).cpu()\n",
    "            # Keep adding these outputs to embeddings.\n",
    "            embeddings = torch.cat((embeddings, enc_output), 0)\n",
    "\n",
    "    # Return the embeddings\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77a34e5-336e-4366-b170-c0f42fc3336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similar_images(image, num_images, embedding, device):\n",
    "    \n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224))])\n",
    "    image_tensor = transform(image).to(device)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model(image_tensor).cpu().detach().numpy()\n",
    "        \n",
    "    flattened_embedding = image_embedding.reshape((image_embedding.shape[0], -1))\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=num_images, metric=\"cosine\")\n",
    "    knn.fit(embedding.reshape((embedding.shape[0], -1)))\n",
    "    _, indices = knn.kneighbors(flattened_embedding)\n",
    "    indices_list = indices.tolist()\n",
    "    \n",
    "    return indices_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc8f30e-703f-4477-8cff-e23e9faf1a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18(nn.Module):\n",
    "    def __init__(self, embedding_size = 500, pretrained=True):\n",
    "        super(Resnet18, self).__init__()\n",
    "\n",
    "        self.model = torchvision.models.resnet18(pretrained = pretrained)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_ftrs = self.model.fc.in_features\n",
    "        \n",
    "        ##implement embedding layer\n",
    "        self.embedding_layer = nn.Linear(self.num_ftrs, self.embedding_size) #500 dim \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        x = self.model.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1) ##flatten\n",
    "        \n",
    "        ##implement embedding layer\n",
    "        x = self.embedding_layer(x) #x 500 dim \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e1f3af-897a-4c50-b829-a116a9485041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure_resnet = torchvision.models.resnet18(pretrained = True)\n",
    "# print(pure_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "629ad9ad-7344-4f29-897b-f2f369b7ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0, num_classes = 200):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, output, labels): ##output shape: [batch, embedding_size], labels shape: [Batch,]\n",
    "        \n",
    "        \n",
    "        ###label matrix y_ij 를 계산하는 다른 방법입니다. 원래 올려드린 ans파일 방법보다 직관적으로 for문을 두번 돌면서 구현한 것 입니다. \n",
    "        ###Tensor, numpy연산의 장점을 살릴려면 원래 올려드린 ans파일처럼 구현하는게 더 좋은 방법입니다. \n",
    "        ##compute label matrix\n",
    "        label_y = []\n",
    "        for i in labels:\n",
    "            ys = []\n",
    "            for j in labels:\n",
    "                if i == j:  ## 같으면 레이블 1\n",
    "                    ys.append(1)\n",
    "                else:  ## 다르면 레이블 0\n",
    "                    ys.append(0)\n",
    "            label_y.append(ys)\n",
    "        \n",
    "        label_y = torch.tensor(label_y).cuda() ##텐서로 바꿔주고 지피유로 올려줌\n",
    "        \n",
    "        ##compute pairwise euclidean distance\n",
    "        euclidean_distance = torch.cdist(output, output, p = 2)\n",
    "        \n",
    "        ##define loss equation\n",
    "        loss_contrastive = torch.mean(label_y * euclidean_distance + (1-label_y) * torch.clamp(self.margin - euclidean_distance, min = 0.0))\n",
    "        \n",
    "        \n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51be47fb-b258-4ee8-a5a3-5bd584d42a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11795\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-921fa2a042f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-abea1ca8b31c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mim_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \"\"\"\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##metric learning losses are defined in pytorch_metric_learning package\n",
    "## many losses are defined in pytorch_metric_learning! try to use other lossses \n",
    "# https://github.com/KevinMusgrave/pytorch-metric-learning/tree/master/src/pytorch_metric_learning/losses\n",
    "\n",
    "from pytorch_metric_learning import miners, losses\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224))]) # Normalize the pixels and convert to tensor.\n",
    "\n",
    "##define dataset\n",
    "train_dataset = CUBirds(\"./\", mode = \"train\", transform = transform)\n",
    "val_dataset = CUBirds(\"./\", mode = \"eval\", transform = transform)\n",
    "\n",
    "# print(len(train_dataset))\n",
    "# print(len(val_dataset))\n",
    "# print(train_dataset[0])\n",
    "full_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset]) # 전체 데이터셋에 대해 image retrieval을 수행하기 위함\n",
    "\n",
    "print(len(full_dataset))\n",
    "\n",
    "# define loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64)\n",
    "full_loader = torch.utils.data.DataLoader(full_dataset, batch_size=64)\n",
    "\n",
    "#define model\n",
    "device = \"cuda:0\"\n",
    "model = Resnet18(500, True).to(device)\n",
    "# print(model)\n",
    "\n",
    "#define optimizer, criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5) \n",
    "\n",
    "##define loss\n",
    "criterion = ContrastiveLoss(margin = 0.5, num_classes = 200)\n",
    "# criterion = losses.ContrastiveLoss(neg_margin = 0.5)\n",
    "\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "# Training Loop\n",
    "for epoch in (range(EPOCHS)):\n",
    "    running_loss = 0.\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        m = model(data.to(device))\n",
    "#         print(m.shape)\n",
    "#         print(target.squeeze())\n",
    "        loss = criterion(m,  target.squeeze().to(device))\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = running_loss/len(train_loader)\n",
    "    \n",
    "    print(f\"Epochs = {epoch}, Training Loss : {train_loss}\")\n",
    "    \n",
    "    ##validation\n",
    "    valid_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            m = model(data.to(device))\n",
    "        \n",
    "            loss = criterion(m, target.squeeze().to(device))\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "    val_loss = valid_loss/len(val_loader)\n",
    "\n",
    "    print(f\"Epochs = {epoch}, Validation Loss : {val_loss}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377942a-b4b2-48a9-9df5-9a61c7b65020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_SHAPE = (1, 512) \n",
    "\n",
    "#get embedding of images from full_loader\n",
    "embedding = create_embedding(model, full_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051fc9a-de09-4734-8d92-cf2c692942ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27046389-bf1d-450a-bdd9-a233711b3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "##find the most closest image to input_image in the full_dataset\n",
    "input_image = PIL.Image.open(\"bird8.jpeg\")\n",
    "indices_list = compute_similar_images(input_image, 20, embedding, device)\n",
    "print(indices_list)\n",
    "\n",
    "for i in indices_list[0]:\n",
    "    img, _ = full_dataset[i]\n",
    "    imshow(img)\n",
    "    import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f795f2f-377f-4d5b-9f5a-b4e15004df52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

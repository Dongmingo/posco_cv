{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Practice\n",
    "- Manual transforms\n",
    "- Custom Dataset\n",
    "- Custom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1636095070674,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "7TOE8d5f1RAx"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fileid: 0BxYys69jI14kRjNmM0gyVWM2bHM\n",
    "# filename: crop_part1.tar.gz\n",
    "\n",
    "!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0BxYys69jI14kRjNmM0gyVWM2bHM' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0BxYys69jI14kRjNmM0gyVWM2bHM\" -O crop_part1.tar.gz && rm -rf ~/cookies.txt\n",
    "!tar -zxvf crop_part1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files list 생성 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data 이상 없는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path preprocess\n",
    "fname = '1_1_0_20161219204750596.jpg.chip.jpg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\n",
    "    './crop_part1/1_1_0_20161219204750596.jpg.chip.jpg').convert('RGB')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torchvision transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.transforms 적용해보기\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation RandomChoice of \n",
    "#       [ Grayscale(),\n",
    "#        CenterCrop(100),\n",
    "#        ColorJitter(0, 0, 0, 0),\n",
    "#        GaussianBlur(kernel_size=9, sigma=(0.1, 10.0)),\n",
    "#        ],\n",
    "#        Resize back to (200,200)\n",
    "\n",
    "tr = \n",
    "\n",
    "tr(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation RandomOrder of \n",
    "#       [ \n",
    "#        ColorJitter(0, 0, 0, 0),\n",
    "#        RandomAffine((0,360),translate=(0.3, 0.4), scale = (0.8,1.2), shear=(-30,30))\n",
    "#        ],\n",
    "#        Resize back to (200,200)\n",
    "\n",
    "tr = \n",
    "\n",
    "tr(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset\n",
    "다음의 세가지 attribute는 반드시 정의\n",
    "- __init__(self, *args, **kargs)\n",
    "- __len__(self)\n",
    "- __getitem__(self, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1636095075960,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "0TKNT_GB1RA0"
   },
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        super().__init__()\n",
    "        # implement this.\n",
    "        # 필요한 정보들 생성.\n",
    "        # Hint: os.listdir(directory) -> List[filename]\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        # implement this.\n",
    "        # 데이터 셋의 크기가 얼마인지? (또는, image가 몇장 있는지)\n",
    "        return \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # implement this\n",
    "        # 인덱스 (idx)를 받아서, 해당 idx 의 data 와 label 을 반환.\n",
    "        # Hint: Image.open(filename).convert('RGB')\n",
    "        # Hint: '1' -> 1. 로 바꾸기 위해선 float() 을 사용.\n",
    "        # Hint: filename = {age}_{gender}_{race}_{time}.jpg\n",
    "        return {\n",
    "            \"image\": img, # [3, 224, 224]\n",
    "            \"age\": age,\n",
    "            \"gender\": gender,\n",
    "            \"race\": race,\n",
    "            \"filename\": filename\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "error",
     "timestamp": 1636095078157,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "uLqm2n9D1RA1",
    "outputId": "c5d5ba40-5550-4472-b348-7734f9344246",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# implement dataset and dataloader, train, test split maybe val set\n",
    "dataset = FaceDataset(\"./crop_part1\", transform)\n",
    "train_length = int(len(dataset) * 0.9)\n",
    "test_length = len(dataset) - train_length\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_length, test_length])\n",
    "print(len(train_set), len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(list_data):\n",
    "#     [dataset.__getitem__(0), ..., dataset.__getitem__(7)]\n",
    "\n",
    "    print(\"My collate function is working!!!!\")\n",
    "    data_dicts = {\n",
    "        \"image\": [], # torch tensor [8, 3, 512, 512]\n",
    "        \"age\": [], # torch tensor [8]\n",
    "        \"gender\": [], # torch tensor [8]\n",
    "        \"race\": [], # torch tensor [8]\n",
    "        \"filename\": [] # torch tensor [8]\n",
    "    }\n",
    "    \n",
    "    batch_size = len(list_data)\n",
    "    for i in range(batch_size):\n",
    "        data = list_data[i] # {\"image\": torch.Tensor, \"age\": float, \"filename\": str}\n",
    "        data_dicts[\"image\"].append(data[\"image\"].unsqueeze(0)) # [1, 3, 224, 224]\n",
    "        data_dicts[\"age\"].append(data[\"age\"])\n",
    "        data_dicts[\"gender\"].append(data[\"gender\"])\n",
    "        data_dicts[\"race\"].append(data[\"race\"])\n",
    "        data_dicts[\"filename\"].append(data[\"filename\"])\n",
    "    data_dicts[\"image\"] = torch.cat(data_dicts[\"image\"], dim=0)\n",
    "    data_dicts[\"age\"] = torch.tensor(data_dicts[\"age\"])\n",
    "    data_dicts[\"gender\"] = torch.tensor(data_dicts[\"gender\"])\n",
    "    data_dicts[\"race\"] = torch.tensor(data_dicts[\"race\"])\n",
    "    return data_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "trainloader = DataLoader(\n",
    "    train_set, batch_size = batch_size, shuffle = True, collate_fn=my_collate_fn\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    test_set, batch_size = batch_size, shuffle = False, collate_fn=my_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "data_dicts = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(data_dicts[\"image\"]))\n",
    "\n",
    "# print labels\n",
    "Gender_dict = {0 : 'Male', 1 : 'Female'}\n",
    "Race_dict = {0 : 'White', 1 : 'Black', 2 : 'Asian', 3: 'Indian', 4 : 'Others'}\n",
    "print('Age    : ', ' '.join('%6s' % str(data_dicts[\"age\"][j].item()) for j in range(8)))\n",
    "print('Gender : ', ' '.join('%6s' % Gender_dict[int(data_dicts[\"gender\"][j].item())] for j in range(8)))\n",
    "print('Race   : ',' '.join('%6s' % Race_dict[int(data_dicts[\"race\"][j].item())] for j in range(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Age Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oW7_lfc51RA1"
   },
   "outputs": [],
   "source": [
    "# ResNet18 pretrained network 받아서 씀\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, use_pretrained=True):\n",
    "        super().__init__()\n",
    "        self.net = models.resnet18(pretrained = use_pretrained) #fc layer 3 는 빼고 가져옴\n",
    "        in_features = self.net.fc.in_features\n",
    "        self.net.fc = nn.Linear(in_features, 1) #512, 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlKuWHM81RA2"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = Net(use_pretrained=True)\n",
    "summary(model, batch_size=-1, input_size=(3, 224, 224), device='cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSAhUAcL1RA2"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for batch_idx, data_dict in enumerate(trainloader):\n",
    "        data = data_dict[\"image\"].to(device)\n",
    "        target = data_dict[\"age\"].to(dtype=torch.float32, device = device).view(-1,1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), total_loss/(batch_idx+1)))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        correct = np.zeros(1)\n",
    "        total = np.zeros(1)\n",
    "        for data_dict in testloader:\n",
    "            target = data_dict[\"age\"].to(dtype=torch.float32, device = device).view(-1,1)\n",
    "            \n",
    "            data, target = data_dict[\"image\"].to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            \n",
    "            correct += np.sum(np.array(target.to('cpu')) == np.round(np.array(output.to('cpu'))), 0)\n",
    "            total += len(target)\n",
    "            \n",
    "        acc = correct/total\n",
    "        test_loss /= len(testloader.dataset)\n",
    "        print(f'\\nTest set: Average loss: {test_loss:.4f}',\n",
    "                f'Correct Labels for Ages : {acc[0]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9H1obP41RA3",
    "outputId": "7fee7717-260c-4dc2-9c66-271a9314b4c5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCnFRBzB1RA4"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "data_dict = dataiter.next()\n",
    "images = data_dict['image']\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GrondTruth: ', ' '.join('%5s' % data_dict['age'][j].item() for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1O0j_7Rr1RA5"
   },
   "outputs": [],
   "source": [
    "images = images.to(device)\n",
    "outputs = model(images)\n",
    "\n",
    "outputs = outputs.squeeze()\n",
    "\n",
    "print('Predicted: ', ' '.join('%.1f' % outputs[j].item() for j in range(8))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Regression model for Age / Gender / Race at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(use_pretrained=True)\n",
    "summary(model, batch_size=-1, input_size=(3, 224, 224), device='cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = \n",
    "optimizer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for batch_idx, data_dict in enumerate(trainloader):\n",
    "        # Implement\n",
    "        data = \n",
    "        label = \n",
    "        \n",
    "        \n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), total_loss/(batch_idx+1)))\n",
    "           \n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        correct = np.zeros(3)\n",
    "        total = np.zeros(3)\n",
    "        for data_dict in testloader:\n",
    "            data = \n",
    "            label = \n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += criterion(output, label).item()\n",
    "            \n",
    "            correct += \n",
    "            total += len(target)\n",
    "            \n",
    "        acc = correct/total\n",
    "        test_loss /= len(testloader.dataset)\n",
    "        print(f'\\nTest set: Average loss: {test_loss:.4f}',\n",
    "                f'Correct Labels for Ages : {acc[0]:.3f}, Gender : {acc[1]:.3f}, Race : {acc[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "data_dict = dataiter.next()\n",
    "images = data_dicts['image']\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('Age    : ', ' '.join('%6s' % str(data_dicts[\"age\"][j].item()) for j in range(8)))\n",
    "print('Gender : ', ' '.join('%6s' % Gender_dict[int(data_dicts[\"gender\"][j].item())] for j in range(8)))\n",
    "print('Race   : ',' '.join('%6s' % Race_dict[int(data_dicts[\"race\"][j].item())] for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.to(device)\n",
    "outputs = model(images)\n",
    "outputs = np.array(outputs.detach().to('cpu'))\n",
    "pred_ages = outputs[:,0]\n",
    "pred_gender = ((outputs[:, 1])>0.5).astype(int)\n",
    "race = outputs[:,2]\n",
    "for idx in range(len(race)):\n",
    "    val = race[idx]\n",
    "    if val <= 0.5:\n",
    "        race[idx] = 0\n",
    "    elif val > 3.5:\n",
    "        race[idx] = 4\n",
    "    else:\n",
    "        race[idx] = np.round(val)\n",
    "pred_race = race.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Age    : ', ' '.join('%6.1f' % pred_ages[j] for j in range(8)))\n",
    "print('Gender : ', ' '.join('%6s' % Gender_dict[pred_gender[j]] for j in range(8)))\n",
    "print('Race   : ',' '.join('%6s' % Race_dict[pred_race[j]] for j in range(8)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Face_Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1636095070674,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "7TOE8d5f1RAx"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fileid: 12XWUcct4LA_bZXaccScYUBC97S9R90L1\n",
    "# filename: crop_part1.tar.gz\n",
    "!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=12XWUcct4LA_bZXaccScYUBC97S9R90L1' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=12XWUcct4LA_bZXaccScYUBC97S9R90L1\" -O crop_part1.tar.gz && rm -rf ~/cookies.txt\n",
    "!tar -zxvf crop_part1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = os.listdir('./crop_part1')\n",
    "print(len(tmp))\n",
    "print(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '1_1_0_20161219204750596.jpg.chip.jpg'\n",
    "splits = fname.split('_')\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\n",
    "    './crop_part1/1_1_0_20161219204750596.jpg.chip.jpg').convert('RGB')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1636095075960,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "0TKNT_GB1RA0"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        # implement this.\n",
    "        # 필요한 정보들 생성.\n",
    "        # Hint: os.listdir(directory) -> List[filename]\n",
    "        self.root_dir = root_dir\n",
    "        self.filenames = os.listdir(root_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        # implement this.\n",
    "        # 데이터 셋의 크기가 얼마인지? (또는, image가 몇장 있는지)\n",
    "        return len(self.filenames)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # implement this\n",
    "        # 인덱스 (idx)를 받아서, 해당 idx 의 data 와 label 을 반환.\n",
    "        # Hint: Image.open(filename).convert('RGB')\n",
    "        # Hint: '1' -> 1. 로 바꾸기 위해선 float() 을 사용.\n",
    "        # Hint: filename = {age}_{gender}_{race}_{time}.jpg\n",
    "        filename = self.filenames[idx]\n",
    "        splits = filename.split('_')\n",
    "        age = float(splits[0])\n",
    "        gender = float(splits[1])\n",
    "        race = float(splits[2])\n",
    "        img = Image.open(os.path.join(self.root_dir, filename)).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return {\n",
    "            \"image\": img, # [3, 224, 224]\n",
    "            \"age\": age,\n",
    "            \"gender\": gender,\n",
    "            \"race\": race,\n",
    "            \"filename\": filename\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "error",
     "timestamp": 1636095078157,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "uLqm2n9D1RA1",
    "outputId": "c5d5ba40-5550-4472-b348-7734f9344246",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "##implement dataset and dataloader\n",
    "dataset = FaceDataset(\"./crop_part1\", transform)\n",
    "train_length = int(len(dataset) * 0.9)\n",
    "test_length = len(dataset) - train_length\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_length, test_length])\n",
    "print(len(train_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate_fn(list_data):\n",
    "#     [dataset.__getitem__(0), ..., dataset.__getitem__(7)]\n",
    "    data_dicts = {\n",
    "        \"image\": [],\n",
    "        \"age\": [],\n",
    "        \"gender\": [],\n",
    "        \"race\": [],\n",
    "        \"filename\": []\n",
    "    }\n",
    "    batch_size = len(list_data)\n",
    "    for i in range(batch_size):\n",
    "        data = list_data[i] # {\"image\": torch.Tensor, \"age\": float, \"filename\": str}\n",
    "        data_dicts[\"image\"].append(data[\"image\"].unsqueeze(0)) # [1, 3, 224, 224]\n",
    "        data_dicts[\"age\"].append(data[\"age\"])\n",
    "        data_dicts[\"gender\"].append(data[\"gender\"])\n",
    "        data_dicts[\"race\"].append(data[\"race\"])\n",
    "        data_dicts[\"filename\"].append(data[\"filename\"])\n",
    "    data_dicts[\"image\"] = torch.cat(data_dicts[\"image\"], dim=0)\n",
    "    data_dicts[\"age\"] = torch.tensor(data_dicts[\"age\"])\n",
    "    data_dicts[\"gender\"] = torch.tensor(data_dicts[\"gender\"])\n",
    "    data_dicts[\"race\"] = torch.tensor(data_dicts[\"race\"])\n",
    "    return data_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "trainloader = DataLoader(\n",
    "    train_set, batch_size = batch_size, shuffle = True, collate_fn=my_collate_fn\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    test_set, batch_size = batch_size, shuffle = False, collate_fn=my_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "data_dicts = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(data_dicts[\"image\"]))\n",
    "\n",
    "# print labels\n",
    "print('Age : ', ' '.join('%5s' % str(data_dicts[\"age\"][j].item()) for j in range(8)))\n",
    "print('Gender : ', ' '.join('%5s' % str(data_dicts[\"gender\"][j].item()) for j in range(8)))\n",
    "print('Race : ',' '.join('%5s' % str(data_dicts[\"race\"][j].item()) for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oW7_lfc51RA1"
   },
   "outputs": [],
   "source": [
    "# ResNet18 pretrained network 받아서 씀\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, use_pretrained=True):\n",
    "        super(Net, self).__init__()\n",
    "        ##implement this\n",
    "        # [3, 224, 224]\n",
    "        self.net = models.resnet18(pretrained = use_pretrained) #fc layer 3 는 빼고 가져옴\n",
    "        in_features = self.net.fc.in_features\n",
    "        self.net.fc = nn.Linear(in_features, 1) #512, 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ##implement this\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.vgg16()\n",
    "print(net.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlKuWHM81RA2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#implement criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = Net(use_pretrained=True)\n",
    "summary(model, batch_size=-1, input_size=(3, 224, 224), device='cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSAhUAcL1RA2"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    for batch_idx, data_dict in enumerate(trainloader):\n",
    "        data = data_dict[\"image\"].to(device)\n",
    "        target = data_dict[\"age\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), total_loss/(batch_idx+1)))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data_dict in testloader:\n",
    "        target = data_dict[\"age\"].type(torch.float).view(-1,1)\n",
    "        data, target = data_dict[\"image\"].to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item() # sum up batch loss\n",
    "\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}'.format(\n",
    "        test_loss, correct, len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9H1obP41RA3",
    "outputId": "7fee7717-260c-4dc2-9c66-271a9314b4c5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCnFRBzB1RA4"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "data_dict = dataiter.next()\n",
    "images = data_dict['image']\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GrondTruth: ', ' '.join('%5s' % data_dict['age'][j].item() for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1O0j_7Rr1RA5"
   },
   "outputs": [],
   "source": [
    "images = images.to(device)\n",
    "outputs = model(images)\n",
    "\n",
    "outputs = outputs.squeeze()\n",
    "\n",
    "print('Predicted: ', ' '.join('%.1f' % outputs[j].item() for j in range(8))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "\n",
    "folder_dir = 'crop_part1'\n",
    "files_list = glob.glob(folder_dir+'/*')\n",
    "for files in files_list:\n",
    "    if len(os.path.split(files)[1].split('_')) != 4:\n",
    "        print(files)\n",
    "        os.remove(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Face_Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YqkKav8O6Fh1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import os.path as osp\n",
    "import collections\n",
    "import PIL\n",
    "import imageio\n",
    "from distutils.version import LooseVersion\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloading dataset from google drive, 밑의 코드의 주석을 풀면 구글 드라이브로 부터 데이터셋 다운, 압축이 풀리고, Kitti라는 폴더가 생성됩니다. \n",
    "\n",
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=19EiycfOQtf6uDKvMgwlHZB50cAxX_U4z' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=19EiycfOQtf6uDKvMgwlHZB50cAxX_U4z\" -O Kitti.zip && rm -rf /tmp/cookies.txt\n",
    "# !mkdir Kitti\n",
    "# !unzip Kitti.zip -d Kitti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsAFBC8X6Fh5"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "error",
     "timestamp": 1636096005436,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "TwA5VAjp6Fh7",
    "outputId": "24ad80d0-4595-45b5-b8c4-c7b3da462966",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/image_2/um_000000.png training/gt_image_2/um_road_000000.png\n",
      "training/image_2/um_000001.png training/gt_image_2/um_road_000001.png\n",
      "training/image_2/um_000002.png training/gt_image_2/um_road_000002.png\n",
      "training/image_2/um_000003.png training/gt_image_2/um_road_000003.png\n",
      "training/image_2/um_000004.png training/gt_image_2/um_road_000004.png\n",
      "training/image_2/um_000005.png training/gt_image_2/um_road_000005.png\n",
      "training/image_2/um_000006.png training/gt_image_2/um_road_000006.png\n",
      "training/image_2/um_000007.png training/gt_image_2/um_road_000007.png\n",
      "training/image_2/um_000008.png training/gt_image_2/um_road_000008.png\n",
      "training/image_2/um_000009.png training/gt_image_2/um_road_000009.png\n",
      "training/image_2/um_000010.png training/gt_image_2/um_road_000010.png\n",
      "training/image_2/um_000011.png training/gt_image_2/um_road_000011.png\n",
      "training/image_2/um_000012.png training/gt_image_2/um_road_000012.png\n",
      "training/image_2/um_000013.png training/gt_image_2/um_road_000013.png\n",
      "training/image_2/um_000014.png training/gt_image_2/um_road_000014.png\n",
      "training/image_2/um_000015.png training/gt_image_2/um_road_000015.png\n",
      "training/image_2/um_000016.png training/gt_image_2/um_road_000016.png\n",
      "training/image_2/um_000017.png training/gt_image_2/um_road_000017.png\n",
      "training/image_2/um_000018.png training/gt_image_2/um_road_000018.png\n",
      "training/image_2/um_000019.png training/gt_image_2/um_road_000019.png\n",
      "training/image_2/um_000020.png training/gt_image_2/um_road_000020.png\n",
      "training/image_2/um_000021.png training/gt_image_2/um_road_000021.png\n",
      "training/image_2/um_000022.png training/gt_image_2/um_road_000022.png\n",
      "training/image_2/um_000023.png training/gt_image_2/um_road_000023.png\n",
      "training/image_2/um_000024.png training/gt_image_2/um_road_000024.png\n",
      "training/image_2/um_000025.png training/gt_image_2/um_road_000025.png\n",
      "training/image_2/um_000026.png training/gt_image_2/um_road_000026.png\n",
      "training/image_2/um_000027.png training/gt_image_2/um_road_000027.png\n",
      "training/image_2/um_000028.png training/gt_image_2/um_road_000028.png\n",
      "training/image_2/um_000029.png training/gt_image_2/um_road_000029.png\n",
      "training/image_2/um_000030.png training/gt_image_2/um_road_000030.png\n",
      "training/image_2/um_000031.png training/gt_image_2/um_road_000031.png\n",
      "training/image_2/um_000032.png training/gt_image_2/um_road_000032.png\n",
      "training/image_2/um_000033.png training/gt_image_2/um_road_000033.png\n",
      "training/image_2/um_000034.png training/gt_image_2/um_road_000034.png\n",
      "training/image_2/um_000035.png training/gt_image_2/um_road_000035.png\n",
      "training/image_2/um_000036.png training/gt_image_2/um_road_000036.png\n",
      "training/image_2/um_000037.png training/gt_image_2/um_road_000037.png\n",
      "training/image_2/um_000038.png training/gt_image_2/um_road_000038.png\n",
      "training/image_2/um_000039.png training/gt_image_2/um_road_000039.png\n",
      "training/image_2/um_000040.png training/gt_image_2/um_road_000040.png\n",
      "training/image_2/um_000041.png training/gt_image_2/um_road_000041.png\n",
      "training/image_2/um_000042.png training/gt_image_2/um_road_000042.png\n",
      "training/image_2/um_000043.png training/gt_image_2/um_road_000043.png\n",
      "training/image_2/um_000044.png training/gt_image_2/um_road_000044.png\n",
      "training/image_2/um_000045.png training/gt_image_2/um_road_000045.png\n",
      "training/image_2/um_000046.png training/gt_image_2/um_road_000046.png\n",
      "training/image_2/um_000047.png training/gt_image_2/um_road_000047.png\n",
      "training/image_2/um_000048.png training/gt_image_2/um_road_000048.png\n",
      "training/image_2/um_000049.png training/gt_image_2/um_road_000049.png\n",
      "training/image_2/um_000050.png training/gt_image_2/um_road_000050.png\n",
      "training/image_2/um_000051.png training/gt_image_2/um_road_000051.png\n",
      "training/image_2/um_000052.png training/gt_image_2/um_road_000052.png\n",
      "training/image_2/um_000053.png training/gt_image_2/um_road_000053.png\n",
      "training/image_2/um_000054.png training/gt_image_2/um_road_000054.png\n",
      "training/image_2/um_000055.png training/gt_image_2/um_road_000055.png\n",
      "training/image_2/um_000056.png training/gt_image_2/um_road_000056.png\n",
      "training/image_2/um_000057.png training/gt_image_2/um_road_000057.png\n",
      "training/image_2/um_000058.png training/gt_image_2/um_road_000058.png\n",
      "training/image_2/um_000059.png training/gt_image_2/um_road_000059.png\n",
      "training/image_2/um_000060.png training/gt_image_2/um_road_000060.png\n",
      "training/image_2/um_000061.png training/gt_image_2/um_road_000061.png\n",
      "training/image_2/um_000062.png training/gt_image_2/um_road_000062.png\n",
      "training/image_2/um_000063.png training/gt_image_2/um_road_000063.png\n",
      "training/image_2/um_000064.png training/gt_image_2/um_road_000064.png\n",
      "training/image_2/um_000065.png training/gt_image_2/um_road_000065.png\n",
      "training/image_2/um_000066.png training/gt_image_2/um_road_000066.png\n",
      "training/image_2/um_000067.png training/gt_image_2/um_road_000067.png\n",
      "training/image_2/um_000068.png training/gt_image_2/um_road_000068.png\n",
      "training/image_2/um_000069.png training/gt_image_2/um_road_000069.png\n",
      "training/image_2/um_000070.png training/gt_image_2/um_road_000070.png\n",
      "training/image_2/um_000071.png training/gt_image_2/um_road_000071.png\n",
      "training/image_2/um_000072.png training/gt_image_2/um_road_000072.png\n",
      "training/image_2/um_000073.png training/gt_image_2/um_road_000073.png\n",
      "training/image_2/um_000074.png training/gt_image_2/um_road_000074.png\n",
      "training/image_2/um_000075.png training/gt_image_2/um_road_000075.png\n",
      "training/image_2/um_000076.png training/gt_image_2/um_road_000076.png\n",
      "training/image_2/um_000077.png training/gt_image_2/um_road_000077.png\n",
      "training/image_2/um_000078.png training/gt_image_2/um_road_000078.png\n",
      "training/image_2/umm_000000.png training/gt_image_2/umm_road_000000.png\n",
      "training/image_2/umm_000001.png training/gt_image_2/umm_road_000001.png\n",
      "training/image_2/umm_000002.png training/gt_image_2/umm_road_000002.png\n",
      "training/image_2/umm_000003.png training/gt_image_2/umm_road_000003.png\n",
      "training/image_2/umm_000004.png training/gt_image_2/umm_road_000004.png\n",
      "training/image_2/umm_000005.png training/gt_image_2/umm_road_000005.png\n",
      "training/image_2/umm_000006.png training/gt_image_2/umm_road_000006.png\n",
      "training/image_2/umm_000007.png training/gt_image_2/umm_road_000007.png\n",
      "training/image_2/umm_000008.png training/gt_image_2/umm_road_000008.png\n",
      "training/image_2/umm_000009.png training/gt_image_2/umm_road_000009.png\n",
      "training/image_2/umm_000010.png training/gt_image_2/umm_road_000010.png\n",
      "training/image_2/umm_000011.png training/gt_image_2/umm_road_000011.png\n",
      "training/image_2/umm_000012.png training/gt_image_2/umm_road_000012.png\n",
      "training/image_2/umm_000013.png training/gt_image_2/umm_road_000013.png\n",
      "training/image_2/umm_000014.png training/gt_image_2/umm_road_000014.png\n",
      "training/image_2/umm_000015.png training/gt_image_2/umm_road_000015.png\n",
      "training/image_2/umm_000016.png training/gt_image_2/umm_road_000016.png\n",
      "training/image_2/umm_000017.png training/gt_image_2/umm_road_000017.png\n",
      "training/image_2/umm_000018.png training/gt_image_2/umm_road_000018.png\n",
      "training/image_2/umm_000019.png training/gt_image_2/umm_road_000019.png\n",
      "training/image_2/umm_000020.png training/gt_image_2/umm_road_000020.png\n",
      "training/image_2/umm_000021.png training/gt_image_2/umm_road_000021.png\n",
      "training/image_2/umm_000022.png training/gt_image_2/umm_road_000022.png\n",
      "training/image_2/umm_000023.png training/gt_image_2/umm_road_000023.png\n",
      "training/image_2/umm_000024.png training/gt_image_2/umm_road_000024.png\n",
      "training/image_2/umm_000025.png training/gt_image_2/umm_road_000025.png\n",
      "training/image_2/umm_000026.png training/gt_image_2/umm_road_000026.png\n",
      "training/image_2/umm_000027.png training/gt_image_2/umm_road_000027.png\n",
      "training/image_2/umm_000028.png training/gt_image_2/umm_road_000028.png\n",
      "training/image_2/umm_000029.png training/gt_image_2/umm_road_000029.png\n",
      "training/image_2/umm_000030.png training/gt_image_2/umm_road_000030.png\n",
      "training/image_2/umm_000031.png training/gt_image_2/umm_road_000031.png\n",
      "training/image_2/umm_000032.png training/gt_image_2/umm_road_000032.png\n",
      "training/image_2/umm_000033.png training/gt_image_2/umm_road_000033.png\n",
      "training/image_2/umm_000034.png training/gt_image_2/umm_road_000034.png\n",
      "training/image_2/umm_000035.png training/gt_image_2/umm_road_000035.png\n",
      "training/image_2/umm_000036.png training/gt_image_2/umm_road_000036.png\n",
      "training/image_2/umm_000037.png training/gt_image_2/umm_road_000037.png\n",
      "training/image_2/umm_000038.png training/gt_image_2/umm_road_000038.png\n",
      "training/image_2/umm_000039.png training/gt_image_2/umm_road_000039.png\n",
      "training/image_2/umm_000040.png training/gt_image_2/umm_road_000040.png\n",
      "training/image_2/umm_000041.png training/gt_image_2/umm_road_000041.png\n",
      "training/image_2/umm_000042.png training/gt_image_2/umm_road_000042.png\n",
      "training/image_2/umm_000043.png training/gt_image_2/umm_road_000043.png\n",
      "training/image_2/umm_000044.png training/gt_image_2/umm_road_000044.png\n",
      "training/image_2/umm_000045.png training/gt_image_2/umm_road_000045.png\n",
      "training/image_2/umm_000046.png training/gt_image_2/umm_road_000046.png\n",
      "training/image_2/umm_000047.png training/gt_image_2/umm_road_000047.png\n",
      "training/image_2/umm_000048.png training/gt_image_2/umm_road_000048.png\n",
      "training/image_2/umm_000049.png training/gt_image_2/umm_road_000049.png\n",
      "training/image_2/umm_000050.png training/gt_image_2/umm_road_000050.png\n",
      "training/image_2/umm_000051.png training/gt_image_2/umm_road_000051.png\n",
      "training/image_2/umm_000052.png training/gt_image_2/umm_road_000052.png\n",
      "training/image_2/umm_000053.png training/gt_image_2/umm_road_000053.png\n",
      "training/image_2/umm_000054.png training/gt_image_2/umm_road_000054.png\n",
      "training/image_2/umm_000055.png training/gt_image_2/umm_road_000055.png\n",
      "training/image_2/umm_000056.png training/gt_image_2/umm_road_000056.png\n",
      "training/image_2/umm_000057.png training/gt_image_2/umm_road_000057.png\n",
      "training/image_2/umm_000058.png training/gt_image_2/umm_road_000058.png\n",
      "training/image_2/umm_000059.png training/gt_image_2/umm_road_000059.png\n",
      "training/image_2/umm_000060.png training/gt_image_2/umm_road_000060.png\n",
      "training/image_2/umm_000061.png training/gt_image_2/umm_road_000061.png\n",
      "training/image_2/umm_000062.png training/gt_image_2/umm_road_000062.png\n",
      "training/image_2/umm_000063.png training/gt_image_2/umm_road_000063.png\n",
      "training/image_2/umm_000064.png training/gt_image_2/umm_road_000064.png\n",
      "training/image_2/umm_000065.png training/gt_image_2/umm_road_000065.png\n",
      "training/image_2/umm_000066.png training/gt_image_2/umm_road_000066.png\n",
      "training/image_2/umm_000067.png training/gt_image_2/umm_road_000067.png\n",
      "training/image_2/umm_000068.png training/gt_image_2/umm_road_000068.png\n",
      "training/image_2/umm_000069.png training/gt_image_2/umm_road_000069.png\n",
      "training/image_2/umm_000070.png training/gt_image_2/umm_road_000070.png\n",
      "training/image_2/umm_000071.png training/gt_image_2/umm_road_000071.png\n",
      "training/image_2/umm_000072.png training/gt_image_2/umm_road_000072.png\n",
      "training/image_2/umm_000073.png training/gt_image_2/umm_road_000073.png\n",
      "training/image_2/umm_000074.png training/gt_image_2/umm_road_000074.png\n",
      "training/image_2/umm_000075.png training/gt_image_2/umm_road_000075.png\n",
      "training/image_2/umm_000076.png training/gt_image_2/umm_road_000076.png\n",
      "training/image_2/umm_000077.png training/gt_image_2/umm_road_000077.png\n",
      "training/image_2/umm_000078.png training/gt_image_2/umm_road_000078.png\n",
      "training/image_2/umm_000079.png training/gt_image_2/umm_road_000079.png\n",
      "training/image_2/uu_000000.png training/gt_image_2/uu_road_000000.png\n",
      "training/image_2/uu_000001.png training/gt_image_2/uu_road_000001.png\n",
      "training/image_2/uu_000002.png training/gt_image_2/uu_road_000002.png\n",
      "training/image_2/uu_000003.png training/gt_image_2/uu_road_000003.png\n",
      "training/image_2/uu_000004.png training/gt_image_2/uu_road_000004.png\n",
      "training/image_2/uu_000005.png training/gt_image_2/uu_road_000005.png\n",
      "training/image_2/uu_000006.png training/gt_image_2/uu_road_000006.png\n",
      "training/image_2/uu_000007.png training/gt_image_2/uu_road_000007.png\n",
      "training/image_2/uu_000008.png training/gt_image_2/uu_road_000008.png\n",
      "training/image_2/uu_000009.png training/gt_image_2/uu_road_000009.png\n",
      "training/image_2/uu_000010.png training/gt_image_2/uu_road_000010.png\n",
      "training/image_2/uu_000011.png training/gt_image_2/uu_road_000011.png\n",
      "training/image_2/uu_000012.png training/gt_image_2/uu_road_000012.png\n",
      "training/image_2/uu_000013.png training/gt_image_2/uu_road_000013.png\n",
      "training/image_2/uu_000014.png training/gt_image_2/uu_road_000014.png\n",
      "training/image_2/uu_000015.png training/gt_image_2/uu_road_000015.png\n",
      "training/image_2/uu_000016.png training/gt_image_2/uu_road_000016.png\n",
      "training/image_2/uu_000017.png training/gt_image_2/uu_road_000017.png\n",
      "training/image_2/uu_000018.png training/gt_image_2/uu_road_000018.png\n",
      "training/image_2/uu_000019.png training/gt_image_2/uu_road_000019.png\n",
      "training/image_2/uu_000020.png training/gt_image_2/uu_road_000020.png\n",
      "training/image_2/uu_000021.png training/gt_image_2/uu_road_000021.png\n",
      "training/image_2/uu_000022.png training/gt_image_2/uu_road_000022.png\n",
      "training/image_2/uu_000023.png training/gt_image_2/uu_road_000023.png\n",
      "training/image_2/uu_000024.png training/gt_image_2/uu_road_000024.png\n",
      "training/image_2/uu_000025.png training/gt_image_2/uu_road_000025.png\n",
      "training/image_2/uu_000026.png training/gt_image_2/uu_road_000026.png\n",
      "training/image_2/uu_000027.png training/gt_image_2/uu_road_000027.png\n",
      "training/image_2/uu_000028.png training/gt_image_2/uu_road_000028.png\n",
      "training/image_2/uu_000029.png training/gt_image_2/uu_road_000029.png\n",
      "training/image_2/uu_000030.png training/gt_image_2/uu_road_000030.png\n",
      "training/image_2/uu_000031.png training/gt_image_2/uu_road_000031.png\n",
      "training/image_2/uu_000032.png training/gt_image_2/uu_road_000032.png\n",
      "training/image_2/uu_000033.png training/gt_image_2/uu_road_000033.png\n",
      "training/image_2/uu_000034.png training/gt_image_2/uu_road_000034.png\n",
      "training/image_2/uu_000035.png training/gt_image_2/uu_road_000035.png\n",
      "training/image_2/uu_000036.png training/gt_image_2/uu_road_000036.png\n",
      "training/image_2/uu_000037.png training/gt_image_2/uu_road_000037.png\n",
      "training/image_2/uu_000038.png training/gt_image_2/uu_road_000038.png\n",
      "training/image_2/uu_000039.png training/gt_image_2/uu_road_000039.png\n",
      "training/image_2/uu_000040.png training/gt_image_2/uu_road_000040.png\n",
      "training/image_2/uu_000041.png training/gt_image_2/uu_road_000041.png\n",
      "training/image_2/uu_000042.png training/gt_image_2/uu_road_000042.png\n",
      "training/image_2/uu_000043.png training/gt_image_2/uu_road_000043.png\n",
      "training/image_2/uu_000044.png training/gt_image_2/uu_road_000044.png\n",
      "training/image_2/uu_000045.png training/gt_image_2/uu_road_000045.png\n",
      "training/image_2/uu_000046.png training/gt_image_2/uu_road_000046.png\n",
      "training/image_2/uu_000047.png training/gt_image_2/uu_road_000047.png\n",
      "training/image_2/uu_000048.png training/gt_image_2/uu_road_000048.png\n",
      "training/image_2/uu_000049.png training/gt_image_2/uu_road_000049.png\n",
      "training/image_2/uu_000050.png training/gt_image_2/uu_road_000050.png\n",
      "training/image_2/uu_000051.png training/gt_image_2/uu_road_000051.png\n",
      "training/image_2/uu_000052.png training/gt_image_2/uu_road_000052.png\n",
      "training/image_2/uu_000053.png training/gt_image_2/uu_road_000053.png\n",
      "training/image_2/uu_000054.png training/gt_image_2/uu_road_000054.png\n",
      "training/image_2/uu_000055.png training/gt_image_2/uu_road_000055.png\n",
      "training/image_2/uu_000056.png training/gt_image_2/uu_road_000056.png\n",
      "training/image_2/uu_000057.png training/gt_image_2/uu_road_000057.png\n",
      "training/image_2/uu_000058.png training/gt_image_2/uu_road_000058.png\n",
      "training/image_2/uu_000059.png training/gt_image_2/uu_road_000059.png\n",
      "training/image_2/uu_000060.png training/gt_image_2/uu_road_000060.png\n",
      "training/image_2/uu_000061.png training/gt_image_2/uu_road_000061.png\n",
      "training/image_2/uu_000062.png training/gt_image_2/uu_road_000062.png\n",
      "training/image_2/uu_000063.png training/gt_image_2/uu_road_000063.png\n",
      "training/image_2/uu_000064.png training/gt_image_2/uu_road_000064.png\n",
      "training/image_2/uu_000065.png training/gt_image_2/uu_road_000065.png\n",
      "training/image_2/uu_000066.png training/gt_image_2/uu_road_000066.png\n",
      "training/image_2/uu_000067.png training/gt_image_2/uu_road_000067.png\n",
      "training/image_2/uu_000068.png training/gt_image_2/uu_road_000068.png\n",
      "training/image_2/uu_000069.png training/gt_image_2/uu_road_000069.png\n",
      "training/image_2/uu_000070.png training/gt_image_2/uu_road_000070.png\n",
      "training/image_2/uu_000071.png training/gt_image_2/uu_road_000071.png\n",
      "training/image_2/uu_000072.png training/gt_image_2/uu_road_000072.png\n",
      "training/image_2/uu_000073.png training/gt_image_2/uu_road_000073.png\n",
      "training/image_2/uu_000074.png training/gt_image_2/uu_road_000074.png\n",
      "training/image_2/uu_000075.png training/gt_image_2/uu_road_000075.png\n",
      "training/image_2/uu_000076.png training/gt_image_2/uu_road_000076.png\n",
      "training/image_2/uu_000077.png training/gt_image_2/uu_road_000077.png\n",
      "training/image_2/uu_000078.png training/gt_image_2/uu_road_000078.png\n",
      "training/image_2/uu_000079.png training/gt_image_2/uu_road_000079.png\n",
      "training/image_2/uu_000080.png training/gt_image_2/uu_road_000080.png\n",
      "training/image_2/uu_000081.png training/gt_image_2/uu_road_000081.png\n"
     ]
    }
   ],
   "source": [
    "imgsets_file = osp.join('Kitti', '{}.txt'.format('train'))\n",
    "for line in open(imgsets_file):\n",
    "    line = line.strip()\n",
    "    print(line)\n",
    "    line = line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vEEnAaqx6Fh8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KITTIdataset(torch.utils.data.Dataset):\n",
    "    class_names = np.array(['background', 'road'])\n",
    "\n",
    "    def __init__(self, root, transform, split='train'): # root: \"./Kitti\"\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_path = []\n",
    "        self.ys = []\n",
    "        \n",
    "        imgsets_file = osp.join(root, '{}.txt'.format(split))\n",
    "        for did in open(imgsets_file):\n",
    "            did = did.strip()\n",
    "            did = did.split()\n",
    "            img_file = osp.join(root, 'data_road/{}'.format(did[0]))\n",
    "            lbl_file = osp.join(root, 'data_road/{}'.format(did[1]))\n",
    "            self.image_path.append(img_file)\n",
    "            self.ys.append(lbl_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image\n",
    "        img_file = self.image_path[index]\n",
    "        img = PIL.Image.open(img_file)\n",
    "        img = np.array(img)\n",
    "        \n",
    "        # load label\n",
    "        lbl_file = self.ys[index]\n",
    "        lbl = PIL.Image.open(lbl_file)\n",
    "        lbl = np.array(lbl)\n",
    "        lbl[lbl == 255] = 1 # 0 is black 255 is white\n",
    "        \n",
    "        return self.transform(img), torch.from_numpy(lbl).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tK9bq2XB6Fh-",
    "outputId": "cd741c18-ad55-43f5-f783-f5def1c72ce4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 1.0000,  1.0000,  1.0000,  ..., -0.1216, -0.2392, -0.2863],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.8353, -0.8196, -0.8039],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.7098, -0.6157, -0.4353],\n",
      "         ...,\n",
      "         [-0.4510, -0.4431, -0.3804,  ..., -0.2549, -0.2549, -0.2471],\n",
      "         [-0.4196, -0.4275, -0.3725,  ..., -0.2549, -0.2627, -0.2706],\n",
      "         [-0.3882, -0.3882, -0.4745,  ..., -0.2235, -0.2392, -0.2549]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  1.0000,  ...,  0.0431,  0.0431, -0.1216],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.4039, -0.4980, -0.4824],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.7725, -0.5922, -0.3020],\n",
      "         ...,\n",
      "         [-0.3490, -0.3569, -0.3725,  ..., -0.2235, -0.2471, -0.2706],\n",
      "         [-0.4196, -0.4431, -0.4510,  ..., -0.2471, -0.2392, -0.2549],\n",
      "         [-0.4510, -0.4353, -0.4118,  ..., -0.2627, -0.2627, -0.2627]],\n",
      "\n",
      "        [[ 1.0000,  1.0000,  1.0000,  ..., -0.6235, -0.5843, -0.5765],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.6471, -0.6549, -0.6784],\n",
      "         [ 1.0000,  1.0000,  1.0000,  ..., -0.8275, -0.8824, -0.8431],\n",
      "         ...,\n",
      "         [-0.2863, -0.3020, -0.3098,  ..., -0.2471, -0.2471, -0.2627],\n",
      "         [-0.4353, -0.4353, -0.4431,  ..., -0.2471, -0.2471, -0.2549],\n",
      "         [-0.4745, -0.4431, -0.4431,  ..., -0.2471, -0.2471, -0.2471]]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "train_dataset = KITTIdataset(root = './Kitti', split = 'train', transform = transform)\n",
    "val_dataset = KITTIdataset(root = './Kitti', split = 'val', transform = transform)\n",
    "\n",
    "print(train_dataset[0])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 1, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOtKS8ll6Fh-"
   },
   "source": [
    "# Define the Network\n",
    "-VGG16\n",
    "\n",
    "- FCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KNXGDhv16Fh_"
   },
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_class = 3):\n",
    "        super(FCN, self).__init__()\n",
    "        \n",
    "        ## Why padding 100?? https://github.com/shelhamer/fcn.berkeleyvision.org\n",
    "        self.features1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding = 1),\n",
    "            nn.ReLU())\n",
    "    \n",
    "        self.features2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding = 1),\n",
    "            nn.ReLU())\n",
    "#         self.pool2 = nn.MaxPool2d(2, stride = 2, ceil_mode = True)    \n",
    "        \n",
    "        self.features3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding = 1))\n",
    "#         self.pool3 = nn.MaxPool2d(2, stride = 2, ceil_mode = True)        \n",
    "        \n",
    "        self.features4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1))\n",
    "#         self.pool4 = nn.MaxPool2d(2, stride = 2, ceil_mode = True)\n",
    "                \n",
    "        self.features5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding = 1))\n",
    "#         self.pool5 = nn.MaxPool2d(2, stride = 2, ceil_mode = True)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2, stride = 2, ceil_mode = True)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(512, 4096, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Conv2d(4096, 4096, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(),\n",
    "            nn.Conv2d(4096, num_class, 1))\n",
    "        \n",
    "        self.upscore2 = nn.ConvTranspose2d(num_class, num_class, kernel_size = 4, stride = 2, bias = False)\n",
    "        self.upscore4 = nn.ConvTranspose2d(num_class, num_class, kernel_size = 4, stride = 2, bias = False)\n",
    "        self.upscore8 = nn.ConvTranspose2d(num_class, num_class, kernel_size = 16, stride = 8, bias = False)\n",
    "        \n",
    "        self.score_pool4 = nn.Conv2d(512, num_class, 1)\n",
    "        self.score_pool3 = nn.Conv2d(256, num_class, 1)\n",
    "        \n",
    "        self.params = [self.features1, self.features2, self.features3, \n",
    "                       self.features4, self.features5]\n",
    "        \n",
    "\n",
    "                             \n",
    "    def forward(self, inputs):\n",
    "        # input [Batch size, 3, w, h]\n",
    "        x = self.features1(inputs) # [Batch size, 64, w, h]\n",
    "        pool1 = self.maxpool(x) # [Batch size, 64, w/2, h/2]\n",
    "        \n",
    "        x = self.features2(pool1) # [Batch size, 128, w/2, h/2]\n",
    "        pool2 = self.maxpool(x) # [Batch size, 128, w/4, h/4]\n",
    "        \n",
    "        x = self.features3(pool2) # [Batch size, 256, w/4, h/4]\n",
    "        pool3 = self.maxpool(x) # [Batch size, 256, w/8, h/8]\n",
    "        \n",
    "        x = self.features4(pool3) # [Batch size, 512, w/8, h/8]\n",
    "        pool4 = self.maxpool(x) # [Batch size, 512, w/16, h/16]\n",
    "        \n",
    "        x = self.features5(pool4) # [Batch size, 512, w/16, h/16]\n",
    "        pool5 = self.maxpool(x) # [Batch size, 512, w/32, h/32]\n",
    "        \n",
    "        x = self.classifier(pool5) # [w/32 - 6, h/32 - 6]\n",
    "        \n",
    "        x = self.upscore2(x) # [w/16 - 10, h/16 - 10]\n",
    "        \n",
    "        pool4 = self.score_pool4(pool4)\n",
    "        pool4 = pool4[:, :, 5:5 + x.size()[2], 5:5 + x.size()[3]]\n",
    "        x = torch.add(x, pool4) # add together \n",
    "        \n",
    "        x = self.upscore4(x) # [w/8 - 18, h/8 - 18]\n",
    "        \n",
    "        pool3 = self.score_pool3(pool3)\n",
    "        pool3 = pool3[:, :, 9:9 + x.size()[2], 9:9 + x.size()[3]]\n",
    "        x = torch.add(x, pool3) # add together\n",
    "\n",
    "        x = self.upscore8(x) \n",
    "        x = x[:, :, 33:33 + inputs.size()[2], 33:33 + inputs.size()[3]]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def copy_params(self, vgg):\n",
    "        for l1, l2 in zip(vgg.features, self.params):\n",
    "            if (isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d)):\n",
    "                assert l1.weight.size() == l2.weight.size()\n",
    "                assert l1.bias.size() == l2.bias.size()\n",
    "                l2.weight.data = l1.weight.data\n",
    "                l2.bias.data = l1.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.,  1.,  2.,  6.,  4.],\n",
      "         [-3.,  1.,  7.,  2., -2.],\n",
      "         [-4.,  2.,  3., -1., -3.],\n",
      "         [-7.,  1.,  2.,  3., 11.],\n",
      "         [ 5., -7.,  8., 12., -9.]]])\n",
      "tensor([[[1., 7.],\n",
      "         [2., 3.]]])\n",
      "tensor([[[ 1.,  7.,  4.],\n",
      "         [ 2.,  3., 11.],\n",
      "         [ 5., 12., -9.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "test = torch.randn(2,3,32,32)\n",
    "x = torch.tensor([[-2, 1, 2, 6, 4], [-3, 1, 7, 2, -2], [-4, 2, 3, -1 , -3], [-7, 1, 2, 3, 11], [5, -7, 8, 12, -9]]).float()\n",
    "x = x.unsqueeze(0)\n",
    "y_1 = nn.MaxPool2d(kernel_size=2,stride=2, padding=0)\n",
    "y_2 = nn.MaxPool2d(kernel_size=2,stride=2, padding=0, ceil_mode=True)\n",
    "print(x)\n",
    "print(y_1(x))\n",
    "print(y_2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i4iwVXSB6FiA",
    "outputId": "32950bd8-0fd5-4a82-ec92-c732da2bde9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 50, 50])\n",
      "torch.Size([1, 2, 408, 408])\n"
     ]
    }
   ],
   "source": [
    "m1 = nn.ConvTranspose2d(2, 2, kernel_size = 4, stride = 2, bias = False)\n",
    "m2 = nn.ConvTranspose2d(2, 2, kernel_size = 4, stride = 2, bias = False)\n",
    "m3 = nn.ConvTranspose2d(2, 2, kernel_size = 16, stride = 8, bias = False)\n",
    "temp_input = torch.randn(1,2,50,50)\n",
    "temp_output = m3(temp_input)\n",
    "print(temp_input.size())\n",
    "print(temp_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4PsFc39F6FiB",
    "outputId": "d707cc55-9778-4d17-96e5-be4f9bd11608"
   },
   "outputs": [],
   "source": [
    "# model = FCN(3)\n",
    "# # print(model)\n",
    "temp_input = torch.rand(1, 3, 1024, 1024)\n",
    "# output = model(temp_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtkrMy4C6FiC"
   },
   "source": [
    "## Measure accuracy and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "5L8zp1sd6FiC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _fast_hist(label_true, label_pred, n_class):\n",
    "    mask = (label_true >= 0) & (label_true < n_class)\n",
    "    hist = np.bincount(\n",
    "        n_class * label_true[mask].astype(int) +\n",
    "        label_pred[mask], minlength=n_class**2).reshape(n_class, n_class)\n",
    "    return hist\n",
    "\n",
    "def compute_mean_iou(label_trues, label_preds, n_class):\n",
    "    hist = np.zeros((n_class, n_class))\n",
    "    for lt, lp in zip(label_trues, label_preds):\n",
    "        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n",
    "    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
    "    mean_iou = np.nanmean(iu)\n",
    "    \n",
    "    return mean_iou\n",
    "\n",
    "def visualization(net, input_img, epoch):\n",
    "    img = transform(input_img).unsqueeze(0)\n",
    "    img = img.cuda()\n",
    "    \n",
    "    score = net(img)\n",
    "#     score = score[\"out\"]\n",
    "    \n",
    "    os.makedirs(\"./pred\", exist_ok = True)\n",
    "    os.makedirs(\"./input\", exist_ok = True)\n",
    "    _, lbl_pred = score.max(1)\n",
    "    lbl_pred = lbl_pred.cpu().numpy()  \n",
    "    lbl_pred = np.squeeze(lbl_pred)\n",
    "    imageio.imsave('./pred/mask_pretrained_'+str(epoch+1)+'.png', lbl_pred)\n",
    "    plt.imshow(mpimg.imread('./pred/mask_pretrained_'+str(epoch+1)+'.png')) ### visualize predicted label map\n",
    "\n",
    "    input_img = np.array(input_img)\n",
    "    imageio.imsave('./input/input_pretrained_'+str(epoch+1)+'.png', input_img)\n",
    "    plt.imshow(mpimg.imread('./input/input_pretrained_'+str(epoch+1)+'.png'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[2 2]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 2. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "[       nan 1.         0.66666667 0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-06631ce91ffc>:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5555555555555555"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_true = np.array([\n",
    "    [1, 2], \n",
    "    [2, 2]\n",
    "])\n",
    "label_pred = np.array([\n",
    "    [1, 2], \n",
    "    [2, 3]\n",
    "])\n",
    "compute_mean_iou(label_true, label_pred, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ES9wHzU6FiD"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCN(\n",
      "  (backbone): IntermediateLayerGetter(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): FCNHead(\n",
      "    (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (aux_classifier): FCNHead(\n",
      "    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ##load pretrained model from torchvision\n",
    "# ##pretrained using coco 2017\n",
    "# net = torchvision.models.segmentation.fcn_resnet50(pretrained = True)\n",
    "# # print(net)\n",
    "# net.classifier = torchvision.models.segmentation.fcn.FCNHead( 2048, 2)\n",
    "# net = net.cuda()\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16 = torchvision.models.vgg16(pretrained = True)\n",
    "# net = FCN(num_class = 2)\n",
    "# net.copy_params(vgg16)\n",
    "\n",
    "# net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5, weight_decay = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "id": "Zqey3Kju6FiD",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "33b61b31-74d3-48a5-9944-ac21804c9bfb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch : 0\n",
      "Best model saved\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.91 GiB total capacity; 4.90 GiB already allocated; 5.38 MiB free; 5.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-aa3034b09bca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m## visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./road_sample1.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mvisualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-7aed402f6437>\u001b[0m in \u001b[0;36mvisualization\u001b[0;34m(net, input_img, epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"out\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/models/segmentation/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# contract: features is a dict of tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2280\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2282\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2283\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.91 GiB total capacity; 4.90 GiB already allocated; 5.38 MiB free; 5.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "training_epochs = 5\n",
    "best_iou = 0\n",
    "num_class = len(train_loader.dataset.class_names)\n",
    "j=0\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    net.train()\n",
    "    print ('current epoch : %d'%(epoch))\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # load data, forward\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        score = net(data)\n",
    "#         import pdb; pdb.set_trace()\n",
    "#         score = score[\"out\"]\n",
    "\n",
    "        loss = criterion(score, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 20 ==0:\n",
    "            print ('batch : {}, loss : {}'.format(batch_idx, loss.item()))\n",
    "        j += 1\n",
    "        \n",
    "    #validation\n",
    "    net.eval()\n",
    "    val_loss = 0\n",
    "    metrics = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            # load data, forward\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            score = net(data)\n",
    "            ## check score output\n",
    "            score = score[\"out\"]\n",
    "\n",
    "            # calc val loss, accuracy\n",
    "            loss = criterion(score, target)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, lbl_pred = score.max(1)\n",
    "            lbl_pred = lbl_pred.cpu().numpy()  \n",
    "            lbl_true = target.cpu().numpy()\n",
    "\n",
    "            for lt, lp in zip(lbl_true, lbl_pred): # lbl_true, lbl_pred: [batch, h, w]\n",
    "                tmp = compute_mean_iou(lt, lp, num_class)\n",
    "                metrics.append(tmp)\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    metrics = np.mean(metrics)\n",
    "    \n",
    "    print ('val loss : {}, mean_iou : {}'.format(val_loss, metrics))\n",
    "\n",
    "    ##save model\n",
    "    ## code 실수\n",
    "    if best_iou < metrics:\n",
    "        best_iou = metrics\n",
    "        print(\"Best model saved\")\n",
    "        torch.save(net.state_dict(), './model_best.pth')\n",
    "    \n",
    "    ## visualization\n",
    "    img = PIL.Image.open('./road_sample1.png')\n",
    "    visualization(net, img, epoch)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = PIL.Image.open('./road_sample1.png')\n",
    "visualization(net, img, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "seg_kitti_ans.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

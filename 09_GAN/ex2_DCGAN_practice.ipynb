{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b27288-2df0-40ca-8e11-b672c02bcf09",
   "metadata": {},
   "source": [
    "# Deep Convloutional GAN (DCGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad4877-2ae8-4fb0-8649-824aff1064f7",
   "metadata": {},
   "source": [
    "- Referecnce : https://github.com/eriklindernoren/PyTorch-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede26885-108e-4524-8acf-52b96f04ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281948d1-f352-4655-8251-e99d0f0d88e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b8f52-d6dc-4722-8000-49e9d19fcc4d",
   "metadata": {},
   "source": [
    "## Use MNIST Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be1559-7ce5-4963-bcc9-908f51675242",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs =128\n",
    "\n",
    "dataset = datasets.MNIST(\"./mnist\", train=True, download=True, \n",
    "                         transform=transforms.Compose([transforms.Resize(32), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=bs, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc9815-ca29-48de-b181-4b6638b5600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "len(batch)\n",
    "print(batch[0].shape, batch[1])\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()\n",
    "    \n",
    "imshow(torchvision.utils.make_grid(batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb42372-adb1-4366-b6b1-c6d5cb022789",
   "metadata": {},
   "source": [
    "## Define Generator & Discriminator\n",
    "### Generator architecture\n",
    "input random vector: 100 dim\n",
    "* Linear: out_features 128 * 8 * 8\n",
    "* BatchNorm2d\n",
    "* Upsample: scale_factor 2\n",
    "* Conv2d: out_channel: 128, kernel size 3, stride 1, padding 1\n",
    "* BatchNorm2d\n",
    "* LeakyReLU: 0.2\n",
    "* Upsample: scale_factor 2\n",
    "* Conv2d: out_channel: 64, kernel size 3, stride 1, padding 1\n",
    "* BatchNorm\n",
    "* LeakyReLU: 0.2\n",
    "* Conv2d: out_channel: 1, kernel size 3, stride 1, padding 1\n",
    "* Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b2f3ac-9e09-473f-ab69-9d9360dad41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #### Implement Here ####\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        #### Implement Here ####\n",
    "        #### Hint : z should be reshaped into 2d before going into bn layer\n",
    "        z = self.fc1(z)\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dede475-d492-4dc9-a1ca-86f5d4628918",
   "metadata": {},
   "source": [
    "### Discriminator architecture\n",
    "input: [1 , 32 , 32] image \n",
    "* Conv2d: out_channel: 16, kernel size 3, stride 2, padding 1\n",
    "* LeakyReLU: 0.2\n",
    "* Dropout: 0.25\n",
    "* Conv2d: out_channel: 32, kernel size 3, stride 2, padding 1\n",
    "* LeakyReLU: 0.2\n",
    "* Dropout: 0.25\n",
    "* BatchNorm2d\n",
    "* Conv2d: out_channel: 64, kernel size 3, stride 2, padding 1\n",
    "* LeakyReLU: 0.2\n",
    "* Dropout: 0.25\n",
    "* BatchNorm2d\n",
    "* Conv2d: out_channel: 128, kernel size 3, stride 2, padding 1\n",
    "* LeakyReLU: 0.2\n",
    "* Dropout: 0.25\n",
    "* BatchNorm2d\n",
    "* Linear: out_features 1\n",
    "* Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bb0e7-928c-42af-81d2-5882061144cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #### Implement Here ####\n",
    "\n",
    "        \n",
    "        \n",
    "    def make_block(self, in_channel, out_channel, bn= True):\n",
    "        #### Implement Here ####\n",
    "        layers = [\n",
    "\n",
    "        ]\n",
    "        if bn:\n",
    "            layers.append()\n",
    "        \n",
    "        return nn.Sequential(*layers) \n",
    "        \n",
    "        \n",
    "    def forward(self, img):# [1, 32, 32]\n",
    "        #### Implement Here ####\n",
    "\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093ae1f-37d9-471a-9017-71e4b88272e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "G = Generator()\n",
    "D = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b944c3-5353-4e52-91df-9c07c1d5bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(G, device = 'cpu', batch_size = -1, input_size = (100,))\n",
    "summary(D, device = 'cpu', batch_size = -1, input_size = (1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e034b-0c5b-4a9e-9a09-90310edeb7d0",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39cc35-2d05-486d-8f55-3ff7dad13db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1: # Conv2d layer weights init\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1: # BatchNorm2d layer weights init\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# weight initialize\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b074a-e5ff-46c3-a9bb-8552045c89cd",
   "metadata": {},
   "source": [
    "## Define loss & Optimizer & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8728f4c-c09b-405a-9712-f8549c0626da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.9999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.9999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161cf88c-c8ad-40ac-8514-9df555ea22af",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97890f-b202-4f96-a13a-8f1bf7dc4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, D = G.to(device), D.to(device)\n",
    "os.makedirs(\"./dcgan_images\", exist_ok=True)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82726823-448c-46d1-9c13-dd4decfae34c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i, (real_imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        real_imgs = real_imgs.to(device)\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = torch.Tensor(np.random.normal(0, 1, (real_imgs.shape[0], 100))).to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        #### Implement Here ####\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        #### Implement Here ####\n",
    "        \n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "\n",
    "        if batches_done % 100 == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "        if batches_done % 2000 == 0:\n",
    "            save_image(gen_imgs.data, \"dcgan_images/%06d.png\" % batches_done, nrow=8, normalize=True)\n",
    "            imshow(torchvision.utils.make_grid(gen_imgs.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a524a9-889f-4728-9345-359ddd3bf332",
   "metadata": {},
   "source": [
    "## Inference\n",
    "- Check noise space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2cc64-d88c-49df-8e67-a033e73ea07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.eval()\n",
    "D.eval()\n",
    "\n",
    "# 랜덤 노이즈(noise) 샘플링\n",
    "z = torch.normal(mean=0, std=1, size=(100, 100)).to(device)\n",
    "\n",
    "# 이미지 생성\n",
    "gen_img = G(z)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(gen_img.detach().cpu(), nrow=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0880397-23f8-459f-8006-d2c1496d6906",
   "metadata": {},
   "source": [
    "### Vector arithmetic \n",
    "- Pick three samples of cluster and take mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aba23c1-7632-49a4-ac43-27ab8ae95c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = (z[0].unsqueeze(0) + z[84].unsqueeze(0) + z[74].unsqueeze(0)) / 3\n",
    "sevens = (z[60].unsqueeze(0) + z[36].unsqueeze(0) + z[32].unsqueeze(0)) / 3\n",
    "nines = (z[4].unsqueeze(0) + z[34].unsqueeze(0) + z[59].unsqueeze(0)) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525adeb3-abd5-4d6e-b4be-9753eb0cff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ones\n",
    "gen_img = G(ones).detach().cpu()\n",
    "plt.imshow(gen_img.squeeze(0).permute(1, 2, 0).squeeze(2), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22516502-3318-45b4-9722-dfc1c13c1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sevens\n",
    "gen_img = G(sevens).detach().cpu()\n",
    "plt.imshow(gen_img.squeeze(0).permute(1, 2, 0).squeeze(2), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ba1c3-6a35-477b-b32c-6111266956b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# twos\n",
    "gen_img = G(nines).detach().cpu()\n",
    "plt.imshow(gen_img.squeeze(0).permute(1, 2, 0).squeeze(2), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e46e1d-c972-4a71-a928-09eb481cf6b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vector_arithmetic\n",
    "gen_vec = \n",
    "gen_img = G(gen_vec).detach().cpu()\n",
    "plt.imshow(gen_img.squeeze(0).permute(1, 2, 0).squeeze(2), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4aeb2a-875f-481e-b97b-120725cb04cf",
   "metadata": {},
   "source": [
    "### Noise Interpolation Visualize\n",
    "- interpolate noise vector and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62674fb0-deb4-4cac-ad56-ee21df35bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inter = 10\n",
    "current = torch.Tensor(size=(num_inter, 100)).to(device)\n",
    "for i in range(num_inter):\n",
    "    current[i] = (ones.squeeze(0) * (i / (num_inter-1)) + nines.squeeze(0) * (((num_inter-1) - i) / (num_inter-1)))\n",
    "\n",
    "gen_img = G(current).detach().cpu()\n",
    "print(gen_img.shape)\n",
    "imshow(torchvision.utils.make_grid(gen_img, nrow=num_inter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6008cfe-2449-4be4-b936-ee6f1e337811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

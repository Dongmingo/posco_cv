{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPAwvZzOLbug"
   },
   "source": [
    "# Training Resnet18 on CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHPi0AZaLbuh"
   },
   "source": [
    "### Load and normalizing the CIFAR10 training and test datasets using torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "error",
     "timestamp": 1637580102894,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "ZcDKUDnyLbuh",
    "outputId": "fe6bf536-06af-4630-c394-6725495beb25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /root/anaconda3/lib/python3.8/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "# Loading and normalizing CIFAR-10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "error",
     "timestamp": 1637579912050,
     "user": {
      "displayName": "Ji Ye Kim",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05110516000762554458"
     },
     "user_tz": -540
    },
    "id": "HdxQcmYbLbui",
    "outputId": "9acc5993-0448-4262-dcbf-101b6d7ae7b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77d7273849045f1a3e1aa744e4d917a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a3ffc42bb14c5aab4efd7e98b16fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3ef662ffd24e19a0b17ee4b06ecaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b53cb5c92a42d0a31e0ae8e0caa83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1641297139223/work/torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose( #transforms 함수 미리 정의\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "#위의 normalize는 는 image = (image - mean) / std 를 시켜줌. 3 개인 이유는 각 채널마다 mean, std를 적용시켜야 되기 때문. \n",
    "#input value [0,1] -> [-1, 1] range로 바뀜. 예를 들어서, minimum value 0은 (0-0.5)/0.5 = -1, maximum value 1은 (1-0.5)/0.5 = 1 이 된다.\n",
    "\n",
    "batch_size = 8\n",
    "#load dataset\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, #torchvision의 dataset에서 CIFAR10 dataset다운 받음. \n",
    "#                                       download=True, transform=transform)\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "#                                       download=True, transform=transform)\n",
    "## define MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root = \"./data\", train = True, transform = transform, download = True)\n",
    "testset = torchvision.datasets.MNIST(root = \"./data\", train = False, transform = transform, download = True)\n",
    "## mnist: 1*28*28\n",
    "#define dataloader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "#            'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "classes = ('0', '1', '2', '3', '4', \n",
    "           '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XHsRuW79Lbuj",
    "outputId": "723186b2-f549-4bfa-afa6-607bd1dbeb27"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3     1     3     4     3     7     2     0\n"
     ]
    }
   ],
   "source": [
    "# display some images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](resnet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    ### implement this\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        ##forward ## conv, bn, relu, conv, bn , addition , relu\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1, stride = 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1, stride = 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size = 1, bias = False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## input channel: 64, out channel: 128\n",
    "        # residual_function -> 128 channel, shortcut 64\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* conv1: 7*7, 64 channels, stride 2, padding 3\n",
    "* maxpool: 3*3, stride 2, padding 1\n",
    "* conv2_1: residual block, in_channels: 64, out_channels: 64\n",
    "* conv2_2: residual block, in_channels: 64, out_channels: 64\n",
    "* conv3_1: residual block, in_channels: 64, out_channels: 128\n",
    "* conv3_2: residual block, in_channels: 128, out_channels: 128\n",
    "* conv4_1: residual block, in_channels: 128, out_channels: 256\n",
    "* conv4_2: residual block, in_channels: 256, out_channels: 256\n",
    "* conv5_1: residual block, in_channels: 256, out_channels: 512\n",
    "* conv5_2: residual block, in_channels: 512, out_channels: 512\n",
    "* average pooling\n",
    "* fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, block, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        ### implement this\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size = 7, stride = 2, padding = 3, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        )\n",
    "        \n",
    "        self.conv2_1 = block(64, 64)\n",
    "        self.conv2_2 = block(64, 64)\n",
    "        \n",
    "        self.conv3_1 = block(64, 128)\n",
    "        self.conv3_2 = block(128, 128)\n",
    "        \n",
    "        self.conv4_1 = block(128, 256)\n",
    "        self.conv4_2 = block(256, 256)\n",
    "        \n",
    "        self.conv5_1 = block(256, 512)\n",
    "        self.conv5_2 = block(512, 512)\n",
    "        \n",
    "        ## average pooling\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # h*w ->1*1\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # x:[batch, 3, 32, 32]\n",
    "        ### implement this\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.conv2_1(x)\n",
    "        x = self.conv2_2(x)\n",
    "        \n",
    "        x = self.conv3_1(x)\n",
    "        x = self.conv3_2(x)\n",
    "\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.conv4_2(x)\n",
    "\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.conv5_2(x) # [batch, 512, h, w]\n",
    "        \n",
    "        x = self.avg_pool(x) # [batch, 512, 1, 1]\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # [batch, 512]\n",
    "        \n",
    "        x = self.fc(x) #[batch, 10]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           3,136\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 8, 8]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 8, 8]             256\n",
      "             ReLU-21            [-1, 128, 8, 8]               0\n",
      "           Conv2d-22            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 8, 8]             256\n",
      "           Conv2d-24            [-1, 128, 8, 8]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 8, 8]             256\n",
      "             ReLU-26            [-1, 128, 8, 8]               0\n",
      "       BasicBlock-27            [-1, 128, 8, 8]               0\n",
      "           Conv2d-28            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 8, 8]             256\n",
      "             ReLU-30            [-1, 128, 8, 8]               0\n",
      "           Conv2d-31            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 8, 8]             256\n",
      "             ReLU-33            [-1, 128, 8, 8]               0\n",
      "       BasicBlock-34            [-1, 128, 8, 8]               0\n",
      "           Conv2d-35            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
      "             ReLU-37            [-1, 256, 8, 8]               0\n",
      "           Conv2d-38            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-39            [-1, 256, 8, 8]             512\n",
      "           Conv2d-40            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-41            [-1, 256, 8, 8]             512\n",
      "             ReLU-42            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-43            [-1, 256, 8, 8]               0\n",
      "           Conv2d-44            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 8, 8]             512\n",
      "             ReLU-46            [-1, 256, 8, 8]               0\n",
      "           Conv2d-47            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-48            [-1, 256, 8, 8]             512\n",
      "             ReLU-49            [-1, 256, 8, 8]               0\n",
      "       BasicBlock-50            [-1, 256, 8, 8]               0\n",
      "           Conv2d-51            [-1, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-53            [-1, 512, 8, 8]               0\n",
      "           Conv2d-54            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-56            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-58            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-59            [-1, 512, 8, 8]               0\n",
      "           Conv2d-60            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-62            [-1, 512, 8, 8]               0\n",
      "           Conv2d-63            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-65            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-66            [-1, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,175,370\n",
      "Trainable params: 11,175,370\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 7.85\n",
      "Params size (MB): 42.63\n",
      "Estimated Total Size (MB): 50.48\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = ResNet18(BasicBlock)\n",
    "summary(net, batch_size=-1, input_size=(1, 32, 32), device='cpu')\n",
    "net = net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv2DHkV4Lbul"
   },
   "source": [
    "### Define a Loss function and optimizer\n",
    "* Use Classification Cross-Entropy loss\n",
    "* Use Adam with learning rate 0.0001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sWsRmnBNLbul"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# define a loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() #CrossEntropyLoss에 softmax까지 구현되어있기 때문에 모델에 softmax를 선언하지 않아도 됨. \n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swvXFumaLbum"
   },
   "source": [
    "### Train the network on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dy5BoEQVLbum",
    "outputId": "9c979e4f-dc1f-40e8-985d-5b6648e38be9"
   },
   "outputs": [],
   "source": [
    "def train(net, optimizer, epoch):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 50 == 49:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "def test(net):\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    \n",
    "    net.eval()\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "        for idx in range(10): \n",
    "            label_idx = (labels == idx)\n",
    "            if label_idx.sum().item() == 0: \n",
    "                continue\n",
    "            class_correct[idx] += labels[label_idx].eq(torch.argmax(outputs[label_idx], 1)).sum().item()\n",
    "            class_total[idx] += labels[label_idx].size(0)\n",
    "\n",
    "    print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
    "    for i in range(10):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 1.324\n",
      "[1,   100] loss: 0.607\n",
      "[1,   150] loss: 0.458\n",
      "[1,   200] loss: 0.556\n",
      "[1,   250] loss: 0.466\n",
      "[1,   300] loss: 0.332\n",
      "[1,   350] loss: 0.299\n",
      "[1,   400] loss: 0.329\n",
      "[1,   450] loss: 0.326\n",
      "[1,   500] loss: 0.204\n",
      "[1,   550] loss: 0.203\n",
      "[1,   600] loss: 0.242\n",
      "[1,   650] loss: 0.278\n",
      "[1,   700] loss: 0.283\n",
      "[1,   750] loss: 0.215\n",
      "[1,   800] loss: 0.150\n",
      "[1,   850] loss: 0.170\n",
      "[1,   900] loss: 0.294\n",
      "[1,   950] loss: 0.230\n",
      "[1,  1000] loss: 0.178\n",
      "[1,  1050] loss: 0.196\n",
      "[1,  1100] loss: 0.201\n",
      "[1,  1150] loss: 0.156\n",
      "[1,  1200] loss: 0.160\n",
      "[1,  1250] loss: 0.203\n",
      "[1,  1300] loss: 0.171\n",
      "[1,  1350] loss: 0.122\n",
      "[1,  1400] loss: 0.169\n",
      "[1,  1450] loss: 0.084\n",
      "[1,  1500] loss: 0.146\n",
      "[1,  1550] loss: 0.218\n",
      "[1,  1600] loss: 0.135\n",
      "[1,  1650] loss: 0.171\n",
      "[1,  1700] loss: 0.172\n",
      "[1,  1750] loss: 0.119\n",
      "[1,  1800] loss: 0.157\n",
      "[1,  1850] loss: 0.141\n",
      "[1,  1900] loss: 0.147\n",
      "[1,  1950] loss: 0.162\n",
      "[1,  2000] loss: 0.113\n",
      "[1,  2050] loss: 0.102\n",
      "[1,  2100] loss: 0.129\n",
      "[1,  2150] loss: 0.117\n",
      "[1,  2200] loss: 0.091\n",
      "[1,  2250] loss: 0.151\n",
      "[1,  2300] loss: 0.105\n",
      "[1,  2350] loss: 0.117\n",
      "[1,  2400] loss: 0.146\n",
      "[1,  2450] loss: 0.110\n",
      "[1,  2500] loss: 0.090\n",
      "[1,  2550] loss: 0.090\n",
      "[1,  2600] loss: 0.113\n",
      "[1,  2650] loss: 0.105\n",
      "[1,  2700] loss: 0.084\n",
      "[1,  2750] loss: 0.172\n",
      "[1,  2800] loss: 0.161\n",
      "[1,  2850] loss: 0.183\n",
      "[1,  2900] loss: 0.101\n",
      "[1,  2950] loss: 0.125\n",
      "[1,  3000] loss: 0.144\n",
      "[1,  3050] loss: 0.088\n",
      "[1,  3100] loss: 0.121\n",
      "[1,  3150] loss: 0.105\n",
      "[1,  3200] loss: 0.146\n",
      "[1,  3250] loss: 0.149\n",
      "[1,  3300] loss: 0.062\n",
      "[1,  3350] loss: 0.098\n",
      "[1,  3400] loss: 0.079\n",
      "[1,  3450] loss: 0.080\n",
      "[1,  3500] loss: 0.071\n",
      "[1,  3550] loss: 0.072\n",
      "[1,  3600] loss: 0.063\n",
      "[1,  3650] loss: 0.139\n",
      "[1,  3700] loss: 0.120\n",
      "[1,  3750] loss: 0.080\n",
      "[1,  3800] loss: 0.090\n",
      "[1,  3850] loss: 0.116\n",
      "[1,  3900] loss: 0.126\n",
      "[1,  3950] loss: 0.108\n",
      "[1,  4000] loss: 0.130\n",
      "[1,  4050] loss: 0.122\n",
      "[1,  4100] loss: 0.098\n",
      "[1,  4150] loss: 0.082\n",
      "[1,  4200] loss: 0.113\n",
      "[1,  4250] loss: 0.116\n",
      "[1,  4300] loss: 0.135\n",
      "[1,  4350] loss: 0.076\n",
      "[1,  4400] loss: 0.107\n",
      "[1,  4450] loss: 0.082\n",
      "[1,  4500] loss: 0.090\n",
      "[1,  4550] loss: 0.123\n",
      "[1,  4600] loss: 0.075\n",
      "[1,  4650] loss: 0.131\n",
      "[1,  4700] loss: 0.084\n",
      "[1,  4750] loss: 0.061\n",
      "[1,  4800] loss: 0.090\n",
      "[1,  4850] loss: 0.047\n",
      "[1,  4900] loss: 0.086\n",
      "[1,  4950] loss: 0.066\n",
      "[1,  5000] loss: 0.134\n",
      "[1,  5050] loss: 0.080\n",
      "[1,  5100] loss: 0.061\n",
      "[1,  5150] loss: 0.113\n",
      "[1,  5200] loss: 0.118\n",
      "[1,  5250] loss: 0.058\n",
      "[1,  5300] loss: 0.123\n",
      "[1,  5350] loss: 0.129\n",
      "[1,  5400] loss: 0.097\n",
      "[1,  5450] loss: 0.088\n",
      "[1,  5500] loss: 0.074\n",
      "[1,  5550] loss: 0.108\n",
      "[1,  5600] loss: 0.109\n",
      "[1,  5650] loss: 0.067\n",
      "[1,  5700] loss: 0.052\n",
      "[1,  5750] loss: 0.109\n",
      "[1,  5800] loss: 0.137\n",
      "[1,  5850] loss: 0.075\n",
      "[1,  5900] loss: 0.054\n",
      "[1,  5950] loss: 0.137\n",
      "[1,  6000] loss: 0.080\n",
      "[1,  6050] loss: 0.054\n",
      "[1,  6100] loss: 0.092\n",
      "[1,  6150] loss: 0.053\n",
      "[1,  6200] loss: 0.089\n",
      "[1,  6250] loss: 0.104\n",
      "[1,  6300] loss: 0.112\n",
      "[1,  6350] loss: 0.091\n",
      "[1,  6400] loss: 0.045\n",
      "[1,  6450] loss: 0.106\n",
      "[1,  6500] loss: 0.047\n",
      "[1,  6550] loss: 0.089\n",
      "[1,  6600] loss: 0.123\n",
      "[1,  6650] loss: 0.090\n",
      "[1,  6700] loss: 0.074\n",
      "[1,  6750] loss: 0.099\n",
      "[1,  6800] loss: 0.097\n",
      "[1,  6850] loss: 0.097\n",
      "[1,  6900] loss: 0.072\n",
      "[1,  6950] loss: 0.122\n",
      "[1,  7000] loss: 0.098\n",
      "[1,  7050] loss: 0.093\n",
      "[1,  7100] loss: 0.039\n",
      "[1,  7150] loss: 0.114\n",
      "[1,  7200] loss: 0.066\n",
      "[1,  7250] loss: 0.096\n",
      "[1,  7300] loss: 0.101\n",
      "[1,  7350] loss: 0.055\n",
      "[1,  7400] loss: 0.083\n",
      "[1,  7450] loss: 0.063\n",
      "[1,  7500] loss: 0.047\n",
      "[2,    50] loss: 0.036\n",
      "[2,   100] loss: 0.037\n",
      "[2,   150] loss: 0.091\n",
      "[2,   200] loss: 0.082\n",
      "[2,   250] loss: 0.079\n",
      "[2,   300] loss: 0.065\n",
      "[2,   350] loss: 0.053\n",
      "[2,   400] loss: 0.080\n",
      "[2,   450] loss: 0.035\n",
      "[2,   500] loss: 0.043\n",
      "[2,   550] loss: 0.044\n",
      "[2,   600] loss: 0.064\n",
      "[2,   650] loss: 0.053\n",
      "[2,   700] loss: 0.077\n",
      "[2,   750] loss: 0.066\n",
      "[2,   800] loss: 0.040\n",
      "[2,   850] loss: 0.042\n",
      "[2,   900] loss: 0.056\n",
      "[2,   950] loss: 0.061\n",
      "[2,  1000] loss: 0.060\n",
      "[2,  1050] loss: 0.054\n",
      "[2,  1100] loss: 0.066\n",
      "[2,  1150] loss: 0.065\n",
      "[2,  1200] loss: 0.036\n",
      "[2,  1250] loss: 0.051\n",
      "[2,  1300] loss: 0.085\n",
      "[2,  1350] loss: 0.080\n",
      "[2,  1400] loss: 0.042\n",
      "[2,  1450] loss: 0.033\n",
      "[2,  1500] loss: 0.108\n",
      "[2,  1550] loss: 0.043\n",
      "[2,  1600] loss: 0.133\n",
      "[2,  1650] loss: 0.070\n",
      "[2,  1700] loss: 0.071\n",
      "[2,  1750] loss: 0.072\n",
      "[2,  1800] loss: 0.047\n",
      "[2,  1850] loss: 0.054\n",
      "[2,  1900] loss: 0.096\n",
      "[2,  1950] loss: 0.059\n",
      "[2,  2000] loss: 0.067\n",
      "[2,  2050] loss: 0.072\n",
      "[2,  2100] loss: 0.079\n",
      "[2,  2150] loss: 0.059\n",
      "[2,  2200] loss: 0.066\n",
      "[2,  2250] loss: 0.074\n",
      "[2,  2300] loss: 0.045\n",
      "[2,  2350] loss: 0.044\n",
      "[2,  2400] loss: 0.095\n",
      "[2,  2450] loss: 0.045\n",
      "[2,  2500] loss: 0.040\n",
      "[2,  2550] loss: 0.030\n",
      "[2,  2600] loss: 0.061\n",
      "[2,  2650] loss: 0.064\n",
      "[2,  2700] loss: 0.087\n",
      "[2,  2750] loss: 0.091\n",
      "[2,  2800] loss: 0.065\n",
      "[2,  2850] loss: 0.039\n",
      "[2,  2900] loss: 0.094\n",
      "[2,  2950] loss: 0.035\n",
      "[2,  3000] loss: 0.087\n",
      "[2,  3050] loss: 0.105\n",
      "[2,  3100] loss: 0.080\n",
      "[2,  3150] loss: 0.056\n",
      "[2,  3200] loss: 0.089\n",
      "[2,  3250] loss: 0.066\n",
      "[2,  3300] loss: 0.052\n",
      "[2,  3350] loss: 0.059\n",
      "[2,  3400] loss: 0.049\n",
      "[2,  3450] loss: 0.089\n",
      "[2,  3500] loss: 0.017\n",
      "[2,  3550] loss: 0.045\n",
      "[2,  3600] loss: 0.066\n",
      "[2,  3650] loss: 0.068\n",
      "[2,  3700] loss: 0.068\n",
      "[2,  3750] loss: 0.091\n",
      "[2,  3800] loss: 0.039\n",
      "[2,  3850] loss: 0.049\n",
      "[2,  3900] loss: 0.081\n",
      "[2,  3950] loss: 0.046\n",
      "[2,  4000] loss: 0.048\n",
      "[2,  4050] loss: 0.053\n",
      "[2,  4100] loss: 0.027\n",
      "[2,  4150] loss: 0.037\n",
      "[2,  4200] loss: 0.022\n",
      "[2,  4250] loss: 0.043\n",
      "[2,  4300] loss: 0.089\n",
      "[2,  4350] loss: 0.047\n",
      "[2,  4400] loss: 0.119\n",
      "[2,  4450] loss: 0.075\n",
      "[2,  4500] loss: 0.048\n",
      "[2,  4550] loss: 0.058\n",
      "[2,  4600] loss: 0.031\n",
      "[2,  4650] loss: 0.073\n",
      "[2,  4700] loss: 0.035\n",
      "[2,  4750] loss: 0.054\n",
      "[2,  4800] loss: 0.056\n",
      "[2,  4850] loss: 0.082\n",
      "[2,  4900] loss: 0.038\n",
      "[2,  4950] loss: 0.053\n",
      "[2,  5000] loss: 0.045\n",
      "[2,  5050] loss: 0.022\n",
      "[2,  5100] loss: 0.072\n",
      "[2,  5150] loss: 0.079\n",
      "[2,  5200] loss: 0.053\n",
      "[2,  5250] loss: 0.044\n",
      "[2,  5300] loss: 0.026\n",
      "[2,  5350] loss: 0.067\n",
      "[2,  5400] loss: 0.036\n",
      "[2,  5450] loss: 0.091\n",
      "[2,  5500] loss: 0.065\n",
      "[2,  5550] loss: 0.046\n",
      "[2,  5600] loss: 0.057\n",
      "[2,  5650] loss: 0.044\n",
      "[2,  5700] loss: 0.080\n",
      "[2,  5750] loss: 0.061\n",
      "[2,  5800] loss: 0.052\n",
      "[2,  5850] loss: 0.043\n",
      "[2,  5900] loss: 0.020\n",
      "[2,  5950] loss: 0.040\n",
      "[2,  6000] loss: 0.013\n",
      "[2,  6050] loss: 0.028\n",
      "[2,  6100] loss: 0.041\n",
      "[2,  6150] loss: 0.031\n",
      "[2,  6200] loss: 0.018\n",
      "[2,  6250] loss: 0.049\n",
      "[2,  6300] loss: 0.078\n",
      "[2,  6350] loss: 0.046\n",
      "[2,  6400] loss: 0.096\n",
      "[2,  6450] loss: 0.032\n",
      "[2,  6500] loss: 0.078\n",
      "[2,  6550] loss: 0.049\n",
      "[2,  6600] loss: 0.072\n",
      "[2,  6650] loss: 0.129\n",
      "[2,  6700] loss: 0.054\n",
      "[2,  6750] loss: 0.032\n",
      "[2,  6800] loss: 0.042\n",
      "[2,  6850] loss: 0.044\n",
      "[2,  6900] loss: 0.017\n",
      "[2,  6950] loss: 0.021\n",
      "[2,  7000] loss: 0.057\n",
      "[2,  7050] loss: 0.057\n",
      "[2,  7100] loss: 0.094\n",
      "[2,  7150] loss: 0.019\n",
      "[2,  7200] loss: 0.031\n",
      "[2,  7250] loss: 0.028\n",
      "[2,  7300] loss: 0.071\n",
      "[2,  7350] loss: 0.087\n",
      "[2,  7400] loss: 0.070\n",
      "[2,  7450] loss: 0.047\n",
      "[2,  7500] loss: 0.057\n",
      "[3,    50] loss: 0.041\n",
      "[3,   100] loss: 0.034\n",
      "[3,   150] loss: 0.081\n",
      "[3,   200] loss: 0.066\n",
      "[3,   250] loss: 0.038\n",
      "[3,   300] loss: 0.036\n",
      "[3,   350] loss: 0.023\n",
      "[3,   400] loss: 0.061\n",
      "[3,   450] loss: 0.029\n",
      "[3,   500] loss: 0.036\n",
      "[3,   550] loss: 0.053\n",
      "[3,   600] loss: 0.040\n",
      "[3,   650] loss: 0.011\n",
      "[3,   700] loss: 0.023\n",
      "[3,   750] loss: 0.049\n",
      "[3,   800] loss: 0.073\n",
      "[3,   850] loss: 0.019\n",
      "[3,   900] loss: 0.074\n",
      "[3,   950] loss: 0.031\n",
      "[3,  1000] loss: 0.039\n",
      "[3,  1050] loss: 0.012\n",
      "[3,  1100] loss: 0.018\n",
      "[3,  1150] loss: 0.031\n",
      "[3,  1200] loss: 0.064\n",
      "[3,  1250] loss: 0.045\n",
      "[3,  1300] loss: 0.045\n",
      "[3,  1350] loss: 0.047\n",
      "[3,  1400] loss: 0.077\n",
      "[3,  1450] loss: 0.048\n",
      "[3,  1500] loss: 0.034\n",
      "[3,  1550] loss: 0.076\n",
      "[3,  1600] loss: 0.037\n",
      "[3,  1650] loss: 0.040\n",
      "[3,  1700] loss: 0.021\n",
      "[3,  1750] loss: 0.032\n",
      "[3,  1800] loss: 0.036\n",
      "[3,  1850] loss: 0.041\n",
      "[3,  1900] loss: 0.047\n",
      "[3,  1950] loss: 0.033\n",
      "[3,  2000] loss: 0.051\n",
      "[3,  2050] loss: 0.027\n",
      "[3,  2100] loss: 0.079\n",
      "[3,  2150] loss: 0.022\n",
      "[3,  2200] loss: 0.087\n",
      "[3,  2250] loss: 0.109\n",
      "[3,  2300] loss: 0.064\n",
      "[3,  2350] loss: 0.061\n",
      "[3,  2400] loss: 0.049\n",
      "[3,  2450] loss: 0.044\n",
      "[3,  2500] loss: 0.038\n",
      "[3,  2550] loss: 0.032\n",
      "[3,  2600] loss: 0.032\n",
      "[3,  2650] loss: 0.051\n",
      "[3,  2700] loss: 0.047\n",
      "[3,  2750] loss: 0.034\n",
      "[3,  2800] loss: 0.044\n",
      "[3,  2850] loss: 0.037\n",
      "[3,  2900] loss: 0.022\n",
      "[3,  2950] loss: 0.032\n",
      "[3,  3000] loss: 0.028\n",
      "[3,  3050] loss: 0.040\n",
      "[3,  3100] loss: 0.035\n",
      "[3,  3150] loss: 0.015\n",
      "[3,  3200] loss: 0.007\n",
      "[3,  3250] loss: 0.024\n",
      "[3,  3300] loss: 0.024\n",
      "[3,  3350] loss: 0.036\n",
      "[3,  3400] loss: 0.026\n",
      "[3,  3450] loss: 0.089\n",
      "[3,  3500] loss: 0.049\n",
      "[3,  3550] loss: 0.080\n",
      "[3,  3600] loss: 0.075\n",
      "[3,  3650] loss: 0.035\n",
      "[3,  3700] loss: 0.048\n",
      "[3,  3750] loss: 0.043\n",
      "[3,  3800] loss: 0.076\n",
      "[3,  3850] loss: 0.037\n",
      "[3,  3900] loss: 0.037\n",
      "[3,  3950] loss: 0.055\n",
      "[3,  4000] loss: 0.027\n",
      "[3,  4050] loss: 0.013\n",
      "[3,  4100] loss: 0.042\n",
      "[3,  4150] loss: 0.032\n",
      "[3,  4200] loss: 0.039\n",
      "[3,  4250] loss: 0.037\n",
      "[3,  4300] loss: 0.057\n",
      "[3,  4350] loss: 0.061\n",
      "[3,  4400] loss: 0.051\n",
      "[3,  4450] loss: 0.037\n",
      "[3,  4500] loss: 0.052\n",
      "[3,  4550] loss: 0.064\n",
      "[3,  4600] loss: 0.052\n",
      "[3,  4650] loss: 0.030\n",
      "[3,  4700] loss: 0.029\n",
      "[3,  4750] loss: 0.038\n",
      "[3,  4800] loss: 0.054\n",
      "[3,  4850] loss: 0.025\n",
      "[3,  4900] loss: 0.049\n",
      "[3,  4950] loss: 0.018\n",
      "[3,  5000] loss: 0.015\n",
      "[3,  5050] loss: 0.058\n",
      "[3,  5100] loss: 0.033\n",
      "[3,  5150] loss: 0.016\n",
      "[3,  5200] loss: 0.058\n",
      "[3,  5250] loss: 0.022\n",
      "[3,  5300] loss: 0.055\n",
      "[3,  5350] loss: 0.045\n",
      "[3,  5400] loss: 0.039\n",
      "[3,  5450] loss: 0.055\n",
      "[3,  5500] loss: 0.039\n",
      "[3,  5550] loss: 0.035\n",
      "[3,  5600] loss: 0.074\n",
      "[3,  5650] loss: 0.011\n",
      "[3,  5700] loss: 0.038\n",
      "[3,  5750] loss: 0.023\n",
      "[3,  5800] loss: 0.041\n",
      "[3,  5850] loss: 0.050\n",
      "[3,  5900] loss: 0.025\n",
      "[3,  5950] loss: 0.029\n",
      "[3,  6000] loss: 0.018\n",
      "[3,  6050] loss: 0.013\n",
      "[3,  6100] loss: 0.038\n",
      "[3,  6150] loss: 0.075\n",
      "[3,  6200] loss: 0.032\n",
      "[3,  6250] loss: 0.029\n",
      "[3,  6300] loss: 0.062\n",
      "[3,  6350] loss: 0.032\n",
      "[3,  6400] loss: 0.026\n",
      "[3,  6450] loss: 0.019\n",
      "[3,  6500] loss: 0.043\n",
      "[3,  6550] loss: 0.009\n",
      "[3,  6600] loss: 0.038\n",
      "[3,  6650] loss: 0.043\n",
      "[3,  6700] loss: 0.025\n",
      "[3,  6750] loss: 0.019\n",
      "[3,  6800] loss: 0.028\n",
      "[3,  6850] loss: 0.044\n",
      "[3,  6900] loss: 0.067\n",
      "[3,  6950] loss: 0.056\n",
      "[3,  7000] loss: 0.030\n",
      "[3,  7050] loss: 0.026\n",
      "[3,  7100] loss: 0.044\n",
      "[3,  7150] loss: 0.021\n",
      "[3,  7200] loss: 0.040\n",
      "[3,  7250] loss: 0.050\n",
      "[3,  7300] loss: 0.071\n",
      "[3,  7350] loss: 0.048\n",
      "[3,  7400] loss: 0.029\n",
      "[3,  7450] loss: 0.023\n",
      "[3,  7500] loss: 0.022\n",
      "[4,    50] loss: 0.006\n",
      "[4,   100] loss: 0.007\n",
      "[4,   150] loss: 0.008\n",
      "[4,   200] loss: 0.022\n",
      "[4,   250] loss: 0.023\n",
      "[4,   300] loss: 0.024\n",
      "[4,   350] loss: 0.019\n",
      "[4,   400] loss: 0.009\n",
      "[4,   450] loss: 0.042\n",
      "[4,   500] loss: 0.017\n",
      "[4,   550] loss: 0.009\n",
      "[4,   600] loss: 0.019\n",
      "[4,   650] loss: 0.075\n",
      "[4,   700] loss: 0.015\n",
      "[4,   750] loss: 0.016\n",
      "[4,   800] loss: 0.064\n",
      "[4,   850] loss: 0.027\n",
      "[4,   900] loss: 0.022\n",
      "[4,   950] loss: 0.033\n",
      "[4,  1000] loss: 0.023\n",
      "[4,  1050] loss: 0.031\n",
      "[4,  1100] loss: 0.046\n",
      "[4,  1150] loss: 0.019\n",
      "[4,  1200] loss: 0.023\n",
      "[4,  1250] loss: 0.034\n",
      "[4,  1300] loss: 0.046\n",
      "[4,  1350] loss: 0.015\n",
      "[4,  1400] loss: 0.031\n",
      "[4,  1450] loss: 0.071\n",
      "[4,  1500] loss: 0.028\n",
      "[4,  1550] loss: 0.035\n",
      "[4,  1600] loss: 0.036\n",
      "[4,  1650] loss: 0.034\n",
      "[4,  1700] loss: 0.011\n",
      "[4,  1750] loss: 0.031\n",
      "[4,  1800] loss: 0.024\n",
      "[4,  1850] loss: 0.026\n",
      "[4,  1900] loss: 0.028\n",
      "[4,  1950] loss: 0.058\n",
      "[4,  2000] loss: 0.028\n",
      "[4,  2050] loss: 0.033\n",
      "[4,  2100] loss: 0.039\n",
      "[4,  2150] loss: 0.009\n",
      "[4,  2200] loss: 0.022\n",
      "[4,  2250] loss: 0.023\n",
      "[4,  2300] loss: 0.035\n",
      "[4,  2350] loss: 0.006\n",
      "[4,  2400] loss: 0.007\n",
      "[4,  2450] loss: 0.020\n",
      "[4,  2500] loss: 0.014\n",
      "[4,  2550] loss: 0.013\n",
      "[4,  2600] loss: 0.034\n",
      "[4,  2650] loss: 0.025\n",
      "[4,  2700] loss: 0.018\n",
      "[4,  2750] loss: 0.026\n",
      "[4,  2800] loss: 0.066\n",
      "[4,  2850] loss: 0.034\n",
      "[4,  2900] loss: 0.016\n",
      "[4,  2950] loss: 0.048\n",
      "[4,  3000] loss: 0.031\n",
      "[4,  3050] loss: 0.029\n",
      "[4,  3100] loss: 0.067\n",
      "[4,  3150] loss: 0.028\n",
      "[4,  3200] loss: 0.048\n",
      "[4,  3250] loss: 0.060\n",
      "[4,  3300] loss: 0.040\n",
      "[4,  3350] loss: 0.024\n",
      "[4,  3400] loss: 0.032\n",
      "[4,  3450] loss: 0.034\n",
      "[4,  3500] loss: 0.015\n",
      "[4,  3550] loss: 0.038\n",
      "[4,  3600] loss: 0.060\n",
      "[4,  3650] loss: 0.059\n",
      "[4,  3700] loss: 0.015\n",
      "[4,  3750] loss: 0.068\n",
      "[4,  3800] loss: 0.048\n",
      "[4,  3850] loss: 0.025\n",
      "[4,  3900] loss: 0.047\n",
      "[4,  3950] loss: 0.011\n",
      "[4,  4000] loss: 0.032\n",
      "[4,  4050] loss: 0.021\n",
      "[4,  4100] loss: 0.043\n",
      "[4,  4150] loss: 0.055\n",
      "[4,  4200] loss: 0.013\n",
      "[4,  4250] loss: 0.017\n",
      "[4,  4300] loss: 0.035\n",
      "[4,  4350] loss: 0.040\n",
      "[4,  4400] loss: 0.019\n",
      "[4,  4450] loss: 0.057\n",
      "[4,  4500] loss: 0.018\n",
      "[4,  4550] loss: 0.033\n",
      "[4,  4600] loss: 0.010\n",
      "[4,  4650] loss: 0.010\n",
      "[4,  4700] loss: 0.042\n",
      "[4,  4750] loss: 0.028\n",
      "[4,  4800] loss: 0.042\n",
      "[4,  4850] loss: 0.026\n",
      "[4,  4900] loss: 0.052\n",
      "[4,  4950] loss: 0.023\n",
      "[4,  5000] loss: 0.007\n",
      "[4,  5050] loss: 0.044\n",
      "[4,  5100] loss: 0.029\n",
      "[4,  5150] loss: 0.010\n",
      "[4,  5200] loss: 0.011\n",
      "[4,  5250] loss: 0.019\n",
      "[4,  5300] loss: 0.015\n",
      "[4,  5350] loss: 0.058\n",
      "[4,  5400] loss: 0.048\n",
      "[4,  5450] loss: 0.024\n",
      "[4,  5500] loss: 0.032\n",
      "[4,  5550] loss: 0.035\n",
      "[4,  5600] loss: 0.031\n",
      "[4,  5650] loss: 0.026\n",
      "[4,  5700] loss: 0.037\n",
      "[4,  5750] loss: 0.055\n",
      "[4,  5800] loss: 0.046\n",
      "[4,  5850] loss: 0.051\n",
      "[4,  5900] loss: 0.041\n",
      "[4,  5950] loss: 0.020\n",
      "[4,  6000] loss: 0.047\n",
      "[4,  6050] loss: 0.024\n",
      "[4,  6100] loss: 0.017\n",
      "[4,  6150] loss: 0.050\n",
      "[4,  6200] loss: 0.031\n",
      "[4,  6250] loss: 0.069\n",
      "[4,  6300] loss: 0.035\n",
      "[4,  6350] loss: 0.032\n",
      "[4,  6400] loss: 0.028\n",
      "[4,  6450] loss: 0.022\n",
      "[4,  6500] loss: 0.035\n",
      "[4,  6550] loss: 0.046\n",
      "[4,  6600] loss: 0.022\n",
      "[4,  6650] loss: 0.010\n",
      "[4,  6700] loss: 0.004\n",
      "[4,  6750] loss: 0.030\n",
      "[4,  6800] loss: 0.032\n",
      "[4,  6850] loss: 0.048\n",
      "[4,  6900] loss: 0.034\n",
      "[4,  6950] loss: 0.008\n",
      "[4,  7000] loss: 0.011\n",
      "[4,  7050] loss: 0.016\n",
      "[4,  7100] loss: 0.019\n",
      "[4,  7150] loss: 0.033\n",
      "[4,  7200] loss: 0.059\n",
      "[4,  7250] loss: 0.047\n",
      "[4,  7300] loss: 0.050\n",
      "[4,  7350] loss: 0.026\n",
      "[4,  7400] loss: 0.028\n",
      "[4,  7450] loss: 0.030\n",
      "[4,  7500] loss: 0.023\n",
      "[5,    50] loss: 0.044\n",
      "[5,   100] loss: 0.057\n",
      "[5,   150] loss: 0.014\n",
      "[5,   200] loss: 0.023\n",
      "[5,   250] loss: 0.005\n",
      "[5,   300] loss: 0.025\n",
      "[5,   350] loss: 0.011\n",
      "[5,   400] loss: 0.021\n",
      "[5,   450] loss: 0.010\n",
      "[5,   500] loss: 0.026\n",
      "[5,   550] loss: 0.022\n",
      "[5,   600] loss: 0.018\n",
      "[5,   650] loss: 0.016\n",
      "[5,   700] loss: 0.015\n",
      "[5,   750] loss: 0.008\n",
      "[5,   800] loss: 0.015\n",
      "[5,   850] loss: 0.021\n",
      "[5,   900] loss: 0.035\n",
      "[5,   950] loss: 0.054\n",
      "[5,  1000] loss: 0.045\n",
      "[5,  1050] loss: 0.025\n",
      "[5,  1100] loss: 0.043\n",
      "[5,  1150] loss: 0.022\n",
      "[5,  1200] loss: 0.013\n",
      "[5,  1250] loss: 0.018\n",
      "[5,  1300] loss: 0.014\n",
      "[5,  1350] loss: 0.024\n",
      "[5,  1400] loss: 0.012\n",
      "[5,  1450] loss: 0.050\n",
      "[5,  1500] loss: 0.034\n",
      "[5,  1550] loss: 0.019\n",
      "[5,  1600] loss: 0.024\n",
      "[5,  1650] loss: 0.022\n",
      "[5,  1700] loss: 0.039\n",
      "[5,  1750] loss: 0.040\n",
      "[5,  1800] loss: 0.022\n",
      "[5,  1850] loss: 0.005\n",
      "[5,  1900] loss: 0.017\n",
      "[5,  1950] loss: 0.014\n",
      "[5,  2000] loss: 0.032\n",
      "[5,  2050] loss: 0.020\n",
      "[5,  2100] loss: 0.013\n",
      "[5,  2150] loss: 0.016\n",
      "[5,  2200] loss: 0.026\n",
      "[5,  2250] loss: 0.016\n",
      "[5,  2300] loss: 0.012\n",
      "[5,  2350] loss: 0.012\n",
      "[5,  2400] loss: 0.032\n",
      "[5,  2450] loss: 0.021\n",
      "[5,  2500] loss: 0.038\n",
      "[5,  2550] loss: 0.033\n",
      "[5,  2600] loss: 0.024\n",
      "[5,  2650] loss: 0.013\n",
      "[5,  2700] loss: 0.027\n",
      "[5,  2750] loss: 0.013\n",
      "[5,  2800] loss: 0.012\n",
      "[5,  2850] loss: 0.060\n",
      "[5,  2900] loss: 0.020\n",
      "[5,  2950] loss: 0.023\n",
      "[5,  3000] loss: 0.016\n",
      "[5,  3050] loss: 0.031\n",
      "[5,  3100] loss: 0.017\n",
      "[5,  3150] loss: 0.009\n",
      "[5,  3200] loss: 0.016\n",
      "[5,  3250] loss: 0.008\n",
      "[5,  3300] loss: 0.011\n",
      "[5,  3350] loss: 0.003\n",
      "[5,  3400] loss: 0.039\n",
      "[5,  3450] loss: 0.053\n",
      "[5,  3500] loss: 0.020\n",
      "[5,  3550] loss: 0.018\n",
      "[5,  3600] loss: 0.010\n",
      "[5,  3650] loss: 0.017\n",
      "[5,  3700] loss: 0.017\n",
      "[5,  3750] loss: 0.027\n",
      "[5,  3800] loss: 0.044\n",
      "[5,  3850] loss: 0.024\n",
      "[5,  3900] loss: 0.044\n",
      "[5,  3950] loss: 0.079\n",
      "[5,  4000] loss: 0.064\n",
      "[5,  4050] loss: 0.025\n",
      "[5,  4100] loss: 0.026\n",
      "[5,  4150] loss: 0.017\n",
      "[5,  4200] loss: 0.035\n",
      "[5,  4250] loss: 0.018\n",
      "[5,  4300] loss: 0.003\n",
      "[5,  4350] loss: 0.025\n",
      "[5,  4400] loss: 0.013\n",
      "[5,  4450] loss: 0.016\n",
      "[5,  4500] loss: 0.029\n",
      "[5,  4550] loss: 0.035\n",
      "[5,  4600] loss: 0.070\n",
      "[5,  4650] loss: 0.017\n",
      "[5,  4700] loss: 0.024\n",
      "[5,  4750] loss: 0.042\n",
      "[5,  4800] loss: 0.008\n",
      "[5,  4850] loss: 0.021\n",
      "[5,  4900] loss: 0.034\n",
      "[5,  4950] loss: 0.022\n",
      "[5,  5000] loss: 0.019\n",
      "[5,  5050] loss: 0.023\n",
      "[5,  5100] loss: 0.026\n",
      "[5,  5150] loss: 0.014\n",
      "[5,  5200] loss: 0.024\n",
      "[5,  5250] loss: 0.020\n",
      "[5,  5300] loss: 0.015\n",
      "[5,  5350] loss: 0.022\n",
      "[5,  5400] loss: 0.012\n",
      "[5,  5450] loss: 0.029\n",
      "[5,  5500] loss: 0.003\n",
      "[5,  5550] loss: 0.008\n",
      "[5,  5600] loss: 0.059\n",
      "[5,  5650] loss: 0.025\n",
      "[5,  5700] loss: 0.032\n",
      "[5,  5750] loss: 0.012\n",
      "[5,  5800] loss: 0.023\n",
      "[5,  5850] loss: 0.016\n",
      "[5,  5900] loss: 0.035\n",
      "[5,  5950] loss: 0.048\n",
      "[5,  6000] loss: 0.048\n",
      "[5,  6050] loss: 0.071\n",
      "[5,  6100] loss: 0.034\n",
      "[5,  6150] loss: 0.019\n",
      "[5,  6200] loss: 0.023\n",
      "[5,  6250] loss: 0.045\n",
      "[5,  6300] loss: 0.025\n",
      "[5,  6350] loss: 0.032\n",
      "[5,  6400] loss: 0.004\n",
      "[5,  6450] loss: 0.006\n",
      "[5,  6500] loss: 0.021\n",
      "[5,  6550] loss: 0.029\n",
      "[5,  6600] loss: 0.040\n",
      "[5,  6650] loss: 0.028\n",
      "[5,  6700] loss: 0.028\n",
      "[5,  6750] loss: 0.059\n",
      "[5,  6800] loss: 0.024\n",
      "[5,  6850] loss: 0.013\n",
      "[5,  6900] loss: 0.024\n",
      "[5,  6950] loss: 0.017\n",
      "[5,  7000] loss: 0.036\n",
      "[5,  7050] loss: 0.020\n",
      "[5,  7100] loss: 0.026\n",
      "[5,  7150] loss: 0.039\n",
      "[5,  7200] loss: 0.012\n",
      "[5,  7250] loss: 0.035\n",
      "[5,  7300] loss: 0.031\n",
      "[5,  7350] loss: 0.031\n",
      "[5,  7400] loss: 0.033\n",
      "[5,  7450] loss: 0.016\n",
      "[5,  7500] loss: 0.013\n",
      "Accuracy of the network on the test images: 99 %\n",
      "Accuracy of     0 : 99 %\n",
      "Accuracy of     1 : 99 %\n",
      "Accuracy of     2 : 99 %\n",
      "Accuracy of     3 : 99 %\n",
      "Accuracy of     4 : 99 %\n",
      "Accuracy of     5 : 98 %\n",
      "Accuracy of     6 : 98 %\n",
      "Accuracy of     7 : 99 %\n",
      "Accuracy of     8 : 98 %\n",
      "Accuracy of     9 : 98 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 5):\n",
    "    train(net, optimizer, epoch)\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWRkA9i3Lbum"
   },
   "source": [
    "### Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6ZxlawuALbum",
    "outputId": "56893162-02e2-40ee-daac-6b7bf4028f57"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GrondTruth:      7     2     1     0     4     1     4     9\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "#print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GrondTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "if116v5sLbun",
    "outputId": "acf6a2d6-3a27-456d-cf2c-49ffb2ed29cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted(net_plain):      7     2     1     0     4     1     4     9\n"
     ]
    }
   ],
   "source": [
    "images = images.cuda()\n",
    "outputs = net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted(net_plain): ', ' '.join('%5s' % classes[predicted[j]] for j in range(batch_size))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6TP4-almLbun"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99 %\n",
      "Accuracy of     0 : 99 %\n",
      "Accuracy of     1 : 99 %\n",
      "Accuracy of     2 : 99 %\n",
      "Accuracy of     3 : 99 %\n",
      "Accuracy of     4 : 99 %\n",
      "Accuracy of     5 : 98 %\n",
      "Accuracy of     6 : 98 %\n",
      "Accuracy of     7 : 99 %\n",
      "Accuracy of     8 : 98 %\n",
      "Accuracy of     9 : 98 %\n"
     ]
    }
   ],
   "source": [
    "test(net)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Exercise1_Training_classifier_on_CIFAR10_Answer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
